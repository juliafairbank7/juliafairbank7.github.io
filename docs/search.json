[
  {
    "objectID": "posts/linear-regression-post/index.html",
    "href": "posts/linear-regression-post/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "link to source code\n\n\nOverview of Linear Regression\nLeast-squares linear regression is a convex linear model that makes predictions of the form \\(\\tilde{y}_i = \\langle w, {x}_{i} \\rangle\\)\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom LinearRegression import LinearRegression\n\nLinReg = LinearRegression()\n\nw0 = -0.5\nw1 =  0.7\n\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1) \n\n\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nplt.scatter(x, y)\n\nLinReg.fit_gradient(x, y)\n\nx_fake = np.linspace(0, 1, 101)[:,np.newaxis]\n\npredictions = LinReg.predict(x_fake)\n\nplt.plot(x_fake, predictions, color = \"black\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\n\nplt.plot(LinReg.score_history, color = \"black\");\n\n\n\n\n\n\nOverview of the Linear Regression Functions\n\n\nfit_analytic()\nKernelLogisticRegression.fit(X, y) is the main function that learns the optimal parameters \\({v}\\). To do this, the function first computes a kernel matrix of X with itself, which is used to minimize the empiricial risk and saved as \\({v}\\).\n\n\nfit_gradient()\nKernelLogisticRegression.fit(X, y) is the main function that learns the optimal parameters \\({v}\\). To do this, the function first computes a kernel matrix of X with itself, which is used to minimize the empiricial risk and saved as \\({v}\\).\n\n\npredict()\nKernelLogisticRegression.predict(X) takes the dot product of the kernel matrix of X with the value \\({v}\\) from the fit() function, then turns dot product value into a binary value.\n\n\nscore()\nKernelLogisticRegression.score(X, y) uses the predict() function to make a prediction matrix of \\({X}\\) and compute the accuracy.\n\n\nDemoing the Linear Regression Algorithm\nLet’s generate a non-lineraly separable dataset to test our algorithm.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n\n\nFrom this dataset, you can start to imagine what the non-linear classification/division might look like. Let’s see how the kernel logisitic regression algorithm attempts to classify that separation.\n\n\nkernel logistic regression\n\nfrom KernelLogisticRegression import KernelLogisticRegression\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {(KLR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nNow, let’s experiment with this algorithm. Because we are using RBF, let’s try changing the value of gamma. As a reminder, a larger gamma means a more wiggly decision boundary.\nLet experiment with a large gamma value and see how the algorithm\n\n\nlarge gamma value\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\nAs you can see, the classifier draws an orange blob around the orange data points rather than drawing a line. This is sufficient to achieve 100% accuracy on the training data.\nHowever, if we were to generate new data, we will see a lower accuracy as the classifier blog remains the same, depiste new data points being generated in different locations.\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nNext, let’s try varying the noise, which is a parameter when we call make_moons. The noise determines how spread out the two crescents of points are. Let’s try changing the amount of noise to see the relationship between noise and gamma."
  },
  {
    "objectID": "posts/logistic-regression-post/index.html",
    "href": "posts/logistic-regression-post/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "overview of logistic regression\ncompare to perceptron\npresent data >> fit it >> predict it >> score it\n\n\nfit()\nLogisticRegression.fit(X, y) is the primary method. When LR.fit(X, y) is called, LR should have an instance variable of weights called w, which is a vector of weights, including the bias term. LR should have an instance variable called LR.loss_history which is a list of the evolution of the loss over the training period, and an instance variable called LR.score_history, which is a list of the evolution of the score over the training period.\n\ndef fit(self, X, y, alpha=0.1, max_epochs=1000):\n    #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    #initialize random w vector\n    self.w_hat = np.random.rand(X_hat.shape[1]) \n        \n    # list of the evolution of the score over the training period\n    self.score_history = []\n    self.loss_history = []\n\n        \n    n = X.shape[0]\n        \n    # compute complete gradient\n    for _ in range(max_epochs):\n            \n        i = np.random.randint(0, n)\n            \n        #update\n        self.w_hat = (\n            self.w_hat \n            - alpha \n            * self.gradient(X, y)\n        )\n            \n        self.score_history.append(self.score(X, y))\n        self.loss_history.append(self.empirical_risk(X, y))\n\n\n\nfit_stochastic()\nLogisticRegression.fit_stochastic(X, y) is an alternative version of the fit() method which computes a stochastic gradient by picking a random subset, computing the stochastic gradient, performing an update, then repeating. When LR.fit_stochastic(X, y) is called, LR should have an instance variable of weights called w, which is a vector of weights, including the bias term b. LR should have an instance variable called LR.loss_history which is a list of the evolution of the loss over the training period, and an instance variable called LR.score_history, which is a list of the evolution of the score over the training period.\n\ndef fit_stochastic(self, X, y, m_epochs=1000, momentum = False, batch_size = 10, alpha = .1):\n        #preprocess X by padding with 1s\n        X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n        #initialize random w vector\n        self.w_hat = np.random.rand(X_hat.shape[1]) \n        \n        # list of the evolution of the score over the training period\n        self.score_history = []\n        self.loss_history = []\n        \n        n = X.shape[0]\n            \n        for j in np.arange(m_epochs):\n            order = np.arange(n)\n            np.random.shuffle(order)\n\n            for batch in np.array_split(order, n // batch_size + 1):\n                x_batch = X[batch,:]\n                y_batch = y[batch]\n                grad = self.gradient(x_batch, y_batch) \n                \n                #update\n                self.w_hat = (\n                self.w_hat \n                - alpha \n                * grad\n                )\n                \n            self.score_history.append(self.score(X, y))\n            self.loss_history.append(self.empirical_risk(X, y))\n\n\n\npredict()\nLogisticRegression.predict(X) should return a vector of predicted labels, which are the model’s predictions for the labels on the data.\n\ndef predict(self, X):\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)            \n    return X_hat@self.w_hat\n\n\n\nscore()\nLogisticRegression.score(X, y) should return the accuracy of the predictions as a number between 0 and 1, with 1 corresponding to perfect classification.\n\ndef score(self, X, y):\n        #preprocess X by padding with 1s\n        X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n        predictions = self.predict(X)\n        \n        accuracy = predictions == y \n        \n        accuracy = accuracy * 1\n        \n        accuracy = accuracy.mean()\n        \n        return accuracy\n\n\n\ngradient()\nLogisticRegression.gradient(X, y) calculates the gradient of the loss function with respect to an instance variable of weights called w.\n\ndef gradient(self, X, y):\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    sum_i = 0;\n    n = X_hat.shape[0]\n        \n    for i in range(n):\n        sum_i += (self.sigmoid(\n            np.dot(self.w_hat, X_hat[i])\n        )\n            - y[i]\n        ) * X_hat[i]\n        \n    grad = 1/n * sum_i\n        \n    return grad\n\n\n\nlogistic_loss()\nLogisticRegression.logistic_loss() calculates the logistic loss using the logistic sigmoid function.\n\ndef logistic_loss(self, y_hat, y): \n        return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n\nempirical_risk()\nLogisticRegression.empirical_risk(X, y, loss, w) calculates the empirical risk using the predict() function.\n\ndef empirical_risk(self, X, y,):\n        y_hat = self.predict(X)\n        return self.logistic_loss(y_hat, y).mean()\n\n\n\ndata set\n\nfrom LogisticRegression import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# inspect the fitted value of w\n#LR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 50, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/kernel-logistic-regression-post/index.html",
    "href": "posts/kernel-logistic-regression-post/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Overview of Kernel Logistic Regression\nRegular logistic regression is a binary classification machine learning algorithm. One weakness of regular logistic regression is that the algorithm doesn’t work well with data that is not linearly separable. Kernel logistic regression is a technique that extends regular logistic regression to be compatible data that is not linearly separable.\nSimilar to other binary classification machine learning algorithms like perceptron and regular logistic regression, kernel logistic regression aims to predict the label of a data point based on some predictor variables.\nKernel logistic regression requires us to specify a kernel function and parameters for the kernel function. I’ll be using the radial basis function (RBF) kernel function. RBF takes in one parameter, gamma, which controls how “wiggly” the decision boundary should be. Larger gamma means a more wiggly decision boundary.\nThe algorithm uses a stochastic gradient descent to train the kernel logistic regression model, which is an iterative process. We can set the number of training iterations as well as the learning rate to control how much the parameters change on each update.\nOnce the kernel logistic regression model is fully trained, there will be an alpha value for each data point and one bias value.\n\n\nOverview of the KLR Functions\n\n\nfit()\nKernelLogisticRegression.fit(X, y) is the main function that learns the optimal parameters \\({v}\\). To do this, the function first computes a kernel matrix of X with itself, which is used to minimize the empiricial risk and saved as \\({v}\\).\n\n\npredict()\nKernelLogisticRegression.predict(X) takes the dot product of the kernel matrix of X with the value \\({v}\\) from the fit() function, then turns dot product value into a binary value.\n\n\nscore()\nKernelLogisticRegression.score(X, y) uses the predict() function to make a prediction matrix of \\({X}\\) and compute the accuracy.\n\n\nUsing the Kernel Logisitic Regression Algorithm\nLet’s generate a non-lineraly separable dataset to test our algorithm.\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(50, shuffle = True, noise = 0.2)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nFrom this dataset, you can start to imagine what the non-linear classification/division might look like. Let’s see how the kernel logisitic regression algorithm attempts to classify that separation.\n\n\nkernel logistic regression\n\nfrom KernelLogisticRegression import KernelLogisticRegression\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {(KLR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nNow, let’s experiment with this algorithm. Because we are using RBF, let’s try changing the value of gamma. As a reminder, a larger gamma means a more wiggly decision boundary.\nLet experiment with a large gamma value and see how the algorithm\n\n\nlarge gamma value\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\nAs you can see, the classifier draws an orange blob around the orange data points rather than drawing a line. This is sufficient to achieve 100% accuracy on the training data.\nHowever, if we were to generate new data, we will see a lower accuracy as the classifier blog remains the same, depiste new data points being generated in different locations.\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nNext, let’s try varying the noise, which is a parameter when we call make_moons. The noise determines how spread out the two crescents of points are. Let’s try changing the amount of noise to see the relationship between noise and gamma."
  },
  {
    "objectID": "posts/perceptron-blog-post/index.html",
    "href": "posts/perceptron-blog-post/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Overview of Perceptron Model\nThe Peceptron Algorithm is a binary, linear classification machine learning algorithm. The algorithms aims to find a rule that separates two distinct groups in some data.\nThe algorithm takes in a vector of n data points that have k features as input and predicts a label by calculating the weighted sum of the inputs and a bias b, and predicts 1 if positive and 0 if negative.\nThe perceptron algorithm aims to find a good (not always the best) \\(\\tilde{w}\\) using the following algorithm:\n\nBegin with some random \\(\\tilde{w}^{(0)}\\)\n“Until we’re done” in some time-step t:\n\nPick a random data point i within the n data points\nCompute \\(\\hat{y}_{i}^{(t)} = \\langle \\tilde{w}^{(t)}, \\tilde{x}_{i} \\rangle\\)\nIf \\(\\hat{y}_{i}^{(t)}{y}_{i}\\) > 0, then point is correctly classified – pass!\nIf \\(\\hat{y}_{i}^{(t)}{y}_{i}\\) < 0, then perform the update:\n\n\\[\n     \\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + {y}_i \\tilde{w}_{i}\n    \\]\n\nIn order to actually separate the two classes, the algorithm uses the prediction function which follows the rule: $ ^{(1)}, _{(i)} _i = 1 $,\nwhich says that if the dot product between \\(\\tilde{w}^{(1)}\\) and \\(\\tilde{w}_{(i)}\\) is less than 0, label it 1, otherwise label it -1.\nLet’s see how my perceptron algorithm works on a set of data.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.gca().set(title = \"Data Points\")\n\n\n\n\n\n\nApplying the fit() method\nPerceptron.fit(X, y) is my primary method. When p.fit(X, y) is called, p should have an instance variable of weights called w. This w is the vector in the classifier above. Additionally, p should have an instance variable called p.history which is a list of the evolution of the score over the training period.\nWithin the fit() function, we perform the perceptron update that correspondes to Equation 1, which states:\n\\[\n\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb1 (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{w}_{(i)} \\rangle \\lt 0)  \\tilde{y}_i \\tilde{x}_i\n\\]\n\n\napplying the predict() method\nPerceptron.predict(X) returns a vector of predicted labels on the data using the function \\[  \\langle \\tilde{w}^{(1)}, \\tilde{w}_{(i)} \\rangle \\lt 0 \\iff {y}_i = 1\n\\]\nSee the linear classification below that separates the two classes.\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#p.score(X, y)\n\n\n\n\n\n\napplying the score() method\nPerceptron.score(X, y) returns the accuracy of the perceptron as a number between 0 and 1, with 1 corresponding to perfect classification. We can see how the algorithm updates through different accuracies/losses, then ultimately converges to 0 with a perfect classification. The algorithm converges at a loss of 0 when it has successfully located a hyperplane that perfectly separates the two classes.\n\n# import Perceptron from source.py\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nHowever, this algorithm only converges when the data is lineraly separable, meaning there exists some line that separates the two classes. So let’s take a look at a nonlineraly separable dataset to see how the algorithm responds.\n\n\nnon-lineraly separable data\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.3, noise=.05)\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs you can see from this datset, there is no straight line that could separate the two classes, making this a nonlinearly separable dataset. Let’s see how the perceptron algorithm fits this data and where it draws the hyperplane.\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs you can see, the line drawn by the algorithm does not perfectly separate the two classes. Let’s look at the loss history graph to see the different updates.\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nAs you can see in the graph above, the algorithm never converges to 0, proving that the algorithm does not work on non-linearly separable datasets.\nNext, let’s see how the algorithm handles datasets with higher dimensions.\n\n\n5-Dimension Data\n\nnp.random.seed(12345)\n\nn = 100\np_features = 5\n\nX_5, y_5 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -3, 4, 1), (1.7, 1.7, 1, 53, 5)])\n\np = Perceptron()\np.fit(X_5, y_5, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nAs you can see in this loss history graph, the algorithm converges to 0 around iteration 65. Because the algorithm eventually has a loss of 0, this means there exists a hyperplane that perfectly separates the two classes. So, the perceptron algorithm works on multi-dimensional data, as long as it is lineraly separable.\n\n\nrun-time complexity()\nFor the update function,\n\\[\n        \\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + {y}_i \\tilde{w}_{i}\n       \\]\nthe run-time complexity of one iteration would be \\({p}^2\\), so it only depends on the number of features, \\({p}\\), and doesn’t depend on the number of datapoints, \\({n}\\)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing least-squares linear regression and experimenting with LASSO regularization for overparameterized problems, CSCI 0451.\n\n\n\n\n\n\nMar 27, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing kernel logistic regression, a method for using linear empirical risk minimization to learn nonlinear decision boundaries, CSCI 0451.\n\n\n\n\n\n\nMar 13, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing simple gradient descent and stochastic gradient descent, comparing their performance for training logistic regression, CSCI 0451.\n\n\n\n\n\n\nMar 6, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets, CSCI 0451.\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog: Julia Fairbank is a senior at Middlebury College pursuing a double major in Computer Science and Philosophy. She created this blog to post about CS 0451: Machine Learning."
  }
]