[
  {
    "objectID": "posts/logistic-regression-post/index.html",
    "href": "posts/logistic-regression-post/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "fit()\nLogisticRegression.fit(X, y) is the primary method. When LR.fit(X, y) is called, LR should have an instance variable of weights called w, which is a vector of weights, including the bias term. LR should have an instance variable called LR.loss_history which is a list of the evolution of the loss over the training period, and an instance variable called LR.score_history, which is a list of the evolution of the score over the training period.\n\ndef fit(self, X, y, alpha=0.1, max_epochs=1000):\n    #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    #initialize random w vector\n    self.w_hat = np.random.rand(X_hat.shape[1]) \n        \n    # list of the evolution of the score over the training period\n    self.score_history = []\n    self.loss_history = []\n\n        \n    n = X.shape[0]\n        \n    # compute complete gradient\n    for _ in range(max_epochs):\n            \n        i = np.random.randint(0, n)\n            \n        #update\n        self.w_hat = (\n            self.w_hat \n            - alpha \n            * self.gradient(X, y)\n        )\n            \n        self.score_history.append(self.score(X, y))\n        self.loss_history.append(self.empirical_risk(X, y))\n\n\n\nfit_stochastic()\nLogisticRegression.fit_stochastic(X, y) is an alternative version of the fit() method which computes a stochastic gradient by picking a random subset, computing the stochastic gradient, performing an update, then repeating. When LR.fit_stochastic(X, y) is called, LR should have an instance variable of weights called w, which is a vector of weights, including the bias term b. LR should have an instance variable called LR.loss_history which is a list of the evolution of the loss over the training period, and an instance variable called LR.score_history, which is a list of the evolution of the score over the training period.\n\ndef fit_stochastic(self, X, y, m_epochs=1000, momentum = False, batch_size = 10, alpha = .1):\n        #preprocess X by padding with 1s\n        X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n        #initialize random w vector\n        self.w_hat = np.random.rand(X_hat.shape[1]) \n        \n        # list of the evolution of the score over the training period\n        self.score_history = []\n        self.loss_history = []\n        \n        n = X.shape[0]\n            \n        for j in np.arange(m_epochs):\n            order = np.arange(n)\n            np.random.shuffle(order)\n\n            for batch in np.array_split(order, n // batch_size + 1):\n                x_batch = X[batch,:]\n                y_batch = y[batch]\n                grad = self.gradient(x_batch, y_batch) \n                \n                #update\n                self.w_hat = (\n                self.w_hat \n                - alpha \n                * grad\n                )\n                \n            self.score_history.append(self.score(X, y))\n            self.loss_history.append(self.empirical_risk(X, y))\n\n\n\npredict()\nLogisticRegression.predict(X) should return a vector of predicted labels, which are the model’s predictions for the labels on the data.\n\ndef predict(self, X):\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)            \n    return X_hat@self.w_hat\n\n\n\nscore()\nLogisticRegression.score(X, y) should return the accuracy of the predictions as a number between 0 and 1, with 1 corresponding to perfect classification.\n\ndef score(self, X, y):\n        #preprocess X by padding with 1s\n        X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n        predictions = self.predict(X)\n        \n        accuracy = predictions == y \n        \n        accuracy = accuracy * 1\n        \n        accuracy = accuracy.mean()\n        \n        return accuracy\n\n\n\ngradient()\nLogisticRegression.gradient(X, y) calculates the gradient of the loss function with respect to an instance variable of weights called w.\n\ndef gradient(self, X, y):\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    sum_i = 0;\n    n = X_hat.shape[0]\n        \n    for i in range(n):\n        sum_i += (self.sigmoid(\n            np.dot(self.w_hat, X_hat[i])\n        )\n            - y[i]\n        ) * X_hat[i]\n        \n    grad = 1/n * sum_i\n        \n    return grad\n\n\n\nlogistic_loss()\nLogisticRegression.logistic_loss() calculates the logistic loss using the logistic sigmoid function.\n\ndef logistic_loss(self, y_hat, y): \n        return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n\nempirical_risk()\nLogisticRegression.empirical_risk(X, y, loss, w) calculates the empirical risk using the predict() function.\n\ndef empirical_risk(self, X, y,):\n        y_hat = self.predict(X)\n        return self.logistic_loss(y_hat, y).mean()\n\n\n\ndata set\n\nfrom LogisticRegression import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# inspect the fitted value of w\n#LR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 50, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/kernel-logistic-regression-post/index.html",
    "href": "posts/kernel-logistic-regression-post/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "link to source code\nhttp://localhost:8948/lab/tree/Desktop/juliafairbank7.github.io/posts/kernel-logistic-regression-post/KernelLogisticRegression.py\n\n\nnon-linear dataset\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\n\n\npre-implemented versions of logistic regression & visualization tool\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\nLR = LogisticRegression()\n\nLR.fit(X, y)\n\nplot_decision_regions(X, y, clf = LR)\n\ntitle = plt.gca().set(title = f\"Accuracy = {(LR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nkernel logistic regression\n\nfrom KernelLogisticRegression import KernelLogisticRegression\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {(KLR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n/Users/juliafairbank/opt/anaconda3/lib/python3.9/site-packages/mlxtend/plotting/decision_regions.py:269: UserWarning: No contour levels were found within the data range.\n  ax.contour(xx, yy, Z, cset.levels, **contour_kwargs)\n\n\n\n\n\n\n\nEXPERIMENTS\n\n\nbasic check, noise = 0.2\n\nfrom KernelLogisticRegression import KernelLogisticRegression # your source code\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\n\n0.5\n\n\n\n\nbasic check, choosing gamma\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n0.5\n\n\n\n\n\n\n\nbasic check, new data\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nexperiment 1, noise = 1.2\n\nX, y = make_moons(200, shuffle = True, noise = 1.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\n\n0.5\n\n\n\n\nexperiment 1, choosing gamma\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n0.5\n\n\n\n\n\n\n\nexperiment 1, new data\n\nX, y = make_moons(200, shuffle = True, noise = 1.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nexperiment 2, noise = 4.2\n\nX, y = make_moons(200, shuffle = True, noise = 4.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\n\n0.5\n\n\n\n\nexperiment 2, choosing gamma\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n0.5\n\n\n\n\n\n\n\nexperiment 2, new data\n\nX, y = make_moons(200, shuffle = True, noise = 4.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")"
  },
  {
    "objectID": "posts/perceptron-blog-post/index.html",
    "href": "posts/perceptron-blog-post/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "fit()\nPerceptron.fit(X, y) is my primary method. When p.fit(X, y) is called, p should have an instance variable of weights called w. This w is the vector in the classifier above. Additionally, p should have an instance variable called p.history which is a list of the evolution of the score over the training period.\n\ndef fit(self, X, y, max_steps=1000):\n    #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n       \n     #make -1 or +1 \n    y_hat = y*2-1\n    \n    #initialize random w vector\n    self.w_hat = np.random.rand(X_hat.shape[1]) \n    self.history = []\n        \n    for _ in range(max_steps):\n    \n        i = np.random.randint(0, X.shape[0])\n            \n        #update\n        self.w_hat = (\n            self.w_hat \n            + ((y_hat[i]*np.dot(self.w_hat, X_hat[i]) <0)*1)\n            * y_hat[i]\n            * X_hat[i]\n        )\n            \n        loss = 1 - self.score(X, y)\n        self.history.append(loss)\n            \n        if loss == 0:\n            break\n\n\n\nperceptron update\nIn the fit() function, I performed the perceptron update that corresponded Equation 1. In the function, I added updated \\(\\hat{w}\\) by adding \\(\\hat{w}\\) to binary value of vector \\(\\hat{y}\\) at instance \\(i\\) multipled by the dot product of \\(\\hat{w}\\) and \\(\\hat{X}\\) at instance \\(i\\), multipled by vector \\(\\hat{y}\\) at instance \\(i\\), multiplied by \\(\\hat{X}\\) at instance \\(i\\).\n\n\npredict()\nPerceptron.predict(X) returns a vector of predicted labels on the data.\n\ndef predict(self, X):\n     #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    prediction_vector = X_hat @ self.w_hat\n        \n    prediction_vector = prediction_vector > 0 #true false\n        \n    prediction_vector = prediction_vector * 1 #1 0\n        \n    return prediction_vector\n\n\n\nscore()\nPerceptron.score(X, y) returns the accuracy of the perceptron as a number between 0 and 1, with 1 corresponding to perfect classification.\n\ndef score(self, X, y):\n    #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    predictions = self.predict(X)\n        \n    accuracy = predictions == y \n        \n    accuracy = accuracy * 1\n        \n    accuracy = accuracy.mean()\n        \n    return accuracy\n\n\n\nEXPERIMENTS\n\n\nlineraly separable data\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.gca().set(title = \"Data Points\")\n\n\n\n\n\n# import Perceptron from source.py\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#p.score(X, y)\n\n\n\n\n\n\nnon-lineraly separable data\nIn this experiment, I am testing the perceptron algorithm on non-lineraly separable data. I chose to create a circular dataset of where one feature is a concentric circle within the other, and could not be separated by a line.\nAs shown by the loss graph below, this experiment proves that the algorithm does not converge on a non-lineraly separable data set.\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.3, noise=.05)\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\n\n\n5-Dimension Data\nIn this experient, I am increasing the dimensions from 2-dimensions to 5-dimensions. As shown below by the loss graph, I am showing that the data remains linterally separable as the function ultimately reaches a 0 zero loss and terminates around iteration 65.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 5\n\nX_5, y_5 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -3, 4, 1), (1.7, 1.7, 1, 53, 5)])\n\n\np = Perceptron()\np.fit(X_5, y_5, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing kernel logistic regression, a method for using linear empirical risk minimization to learn nonlinear decision boundaries, CSCI 0451.\n\n\n\n\n\n\nMar 13, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing simple gradient descent and stochastic gradient descent, comparing their performance for training logistic regression, CSCI 0451.\n\n\n\n\n\n\nMar 6, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets, CSCI 0451.\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog: Julia Fairbank is a senior at Middlebury College pursuing a double major in Computer Science and Philosophy. She created this blog to post about CS 0451: Machine Learning."
  }
]