[
  {
    "objectID": "mid-course.html",
    "href": "mid-course.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "Julia Fairbank\n\n\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? I’ve missed 2 classes due to being sick, communicated ahead of time.\nHow often have you taken notes on the core readings ahead of the class period? ~60% of the time, I read/skim mostly and take notes in class.\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? ~85% of the time.\nHow many times have you actually presented the daily warm-up to your team? 3 times (3 called for, 1 stepping in).\nHow many times have you asked your team for help while presenting the daily warm-up? Never.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Most days! One of my teammates is a math major and I’ve noticed that he approaches the problems in way I didn’t consider.\nHow often have you helped a teammate during the daily warm-up presentation? A few times!\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? I try to attend Peer Help every week (twice if I have the time!).\n\nHow often have you asked for or received help from your fellow students? A few times, especially on warm-ups or implementing equations for blog posts.\nHave you been regularly participating in a study group outside class? No. \nHow often have you posted questions or answers in Slack? Not much, I prefer going to Peer Help.\n\n\n\n\n\nHow many blog posts have you submitted? 4 (Linear Regression still being a major WIP).\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: 0\nRevisions useful: 1\nRevisions encouraged: 2\nIncomplete: 1\n\nRoughly how many hours per week have you spent on this course outside of class? ~9.\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI chose to focus on implementation, and have been active in attending Peer Help to try to make my code as clean and effective as possible. It takes longer to figure out how to do the work with matrix math as opposed to loops but it makes my code so much more legible and saves me a lot of time in the future.\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nOne of my goals from the beginning of the semester wewasre to submit an attempt on at least all of the blog posts, and submit at least one revision using Peer Help or Student Hours. So far, I have submitted attempts on 4 blog posts, only one of which is at Revision Useful, two being at Revisions Encouraged, and one that hasn’t been graded (and is also a work in progress). However, I’ve realized that spending more time putting together a comprehensible, complete blog post is more valuable than submitting as many submissions as I can. In that sense, I think I will prioritize bulking up and revising the 4 blog posts I currently have submitted before moving on to the next one.\n\n\n\nOne goal that I am proud to say I have kept up with is my attendance in class and Peer Help. I haven’t attended Student Hours yet because my schedule during the day doens’t work well with the hours, but the late night sessions on Mondays and Thursdays work well and I’ve found a lot of success from attending those sessions. I’ve been able to attend the majority of our classes and have kept up with most of the warmups.\n\n\n\nOne of the warmups that I wasn’t able to do was the Project Proposal. Luckily, in the beginning of the semester, I pulled a few data sets and articles of project ideas that I could potentially be interested in, so I was able to talk about a few of those with my teammates during the warmup and discuss their feasibility. I was really excited to hear about David’s project and spoke to him after about joining a group with him and Cece. I think we are a little behind but meeting later today, Wednesday April 5th, to get things moving.\nBecause Cece is in a different section, I think we will have to work extra hard to make sure we are all on the same page and have consistent meeting times.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nOne piece of context that I’ve shared with you is that I am currently in the weeds with my MBA application. I’ve been spending majority of my time putting together my written responses and preparing for the GMAT. This wasn’t something I was sure I was going to do this semester, so my goals for this class didn’t take into account this additional workload. For full transparency, this application will be my top priority for the next 2-3 weeks (final application due April 27th), meaning that my courses will be less of a priority and I will have less time to dedicate to them.\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nGoal 1: 1. Submit an attempt on at least every blog post. 2. Of the 5 (+1 additional) blog posts that are posted currently, I have submitted attempts on 4 of them. 3. I would like to revise this goal to have at least 3 blog posts with “No Revisions Suggested,” and attempts on the majority of the remaining blog posts. I no longer expect/am aiming for submitting attempts on every blog post, and would rather spend more time making sure that I have strong submissions rather than many submissions.\nGoal 2: 1. Attempt to revise each blog post. 2. Of the 4 blog posts I have submitted, I am currently in revision for all 4. 3. I would like to keep this goal because it supports my bigger goal of having stronger submissions over many submissions. This goal will also help me to revise my posts up to my first goal of having at least 3 blog posts with “No Revisions Suggested.”\nGoal 3: 1. Make strong attempts on all of the warm-ups. 2. I have made attempts on ~85% of the warm-ups, missing a few when I had too much work for other classes and wasn’t able to get to them on time. 3. I would like to continue being prepared for class and be a reliable teammate, but acknowledge that I don’t always have time to attempt every warm-up to my full capacity. Considering this, I’d like to spend at least 20 mins on every warm-up, regardless of if I’m able to complete any of it, just so I understand what is being asked and what questions I have to bring to my group. Then, I will be able to be an active participant in the warm-ip activity, even if I don’t have a final answer.\nGoal 4: 1. Attend all the classes that I am physically able to. 2. I have attended all but two classes, both of which I had to miss due to sickness. I communicated ahead of class and coordinated with my teammates to get the notes that I missed from class. 3. I would like to keep this goal as is!\nGoal 5: 1. Attend Student Hours/Peer Help frequently. 2. I try to attend Peer Help weekly and have benefited greatly from working with Steven. 3. I would like to keep this goal up but with an emphasis on Peer Help since those hours work better with my schedule.\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far. Here are some soundbytes to help guide your thinking:\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A-\n\n\n\n\n\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\nI am excited about how much I’ve learned in this class so far! It’s definitely a challenging course for me, but I feel proud of the work I’ve put in on my own, in class, and in Peer Help hours. I am a little disappointed with where I am at numbers wise (having 4 points, 2 on Perceptron + 1 on Gradient Descent + 1 on Logisitic Regression + 0 on Linear Regression) but have shifted my goals to prioritize revisions over getting attempts on my blog posts. I hope by the end of the year I am able to get up to my revised goals and have stronger blog posts that accurately reflect the time and work I put into them."
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html",
    "href": "posts/Auditing Allocative Bias/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#how-many-individuals-are-in-the-dataframe",
    "href": "posts/Auditing Allocative Bias/index.html#how-many-individuals-are-in-the-dataframe",
    "title": "Auditing Allocative Bias",
    "section": "1. How many individuals are in the dataframe?",
    "text": "1. How many individuals are in the dataframe?\n\ntotal = len(df.index)\ntotal\n\n8268\n\n\nThere are 8,268 individuals in this dataset."
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#what-proportion-have-a-target-label-equal-to-1",
    "href": "posts/Auditing Allocative Bias/index.html#what-proportion-have-a-target-label-equal-to-1",
    "title": "Auditing Allocative Bias",
    "section": "2. What proportion have a target label equal to 1?",
    "text": "2. What proportion have a target label equal to 1?\n\ntotal_ = len(df[df['ESR_label'] == 1])\nproportion_ = total_ / total\nproportion_\n\n0.45379777455249154\n\n\nOf those individuals, the proportion of people that have a target label equal to 1 (employed individuals) is 45.3798%."
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#of-these-individuals-how-many-are-in-each-of-the-groups",
    "href": "posts/Auditing Allocative Bias/index.html#of-these-individuals-how-many-are-in-each-of-the-groups",
    "title": "Auditing Allocative Bias",
    "section": "3. Of these individuals, how many are in each of the groups?",
    "text": "3. Of these individuals, how many are in each of the groups?\n\nlabel1 = df[df['ESR_label'] == 1]\nlabel1_groups = label1.groupby(['RACE'])['RACE'].count()\nprint(label1_groups)\n\nRACE\n1    3460\n2       8\n3     166\n5       5\n6      21\n7       2\n8      13\n9      77\nName: RACE, dtype: int64\n\n\nSee the values above to see how many individuals that have a target label = 1 are in each of the groups.\nFor reference, - Race 1: White alone - Race 2 Black or African American alone - Race 3: American Indian alone - Race 4: Alaska Native alone - Race 5: American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races - Race 6: Asian alone - Race 7: Native Hawaiian and Other Pacific Islander alone - Race 8: Some Other Race alone - Race 9: Two or More Races"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#in-each-group-what-proportion-of-individuals-have-target-label-equal-to-1",
    "href": "posts/Auditing Allocative Bias/index.html#in-each-group-what-proportion-of-individuals-have-target-label-equal-to-1",
    "title": "Auditing Allocative Bias",
    "section": "4. In each group, what proportion of individuals have target label equal to 1?",
    "text": "4. In each group, what proportion of individuals have target label equal to 1?\n\nlabel1_groups_ratio = df.groupby(['RACE'])['ESR_label'].mean()\nprint(label1_groups_ratio)\n\nRACE\n1    0.471005\n2    0.470588\n3    0.286701\n4    0.000000\n5    0.200000\n6    0.500000\n7    0.500000\n8    0.333333\n9    0.358140\nName: ESR_label, dtype: float64"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#check-for-intersectional-trends-by-studying-the-proportion-of-positive-target-labels-broken-out-by-your-chosen-group-labels-and-an-additional-group-label.",
    "href": "posts/Auditing Allocative Bias/index.html#check-for-intersectional-trends-by-studying-the-proportion-of-positive-target-labels-broken-out-by-your-chosen-group-labels-and-an-additional-group-label.",
    "title": "Auditing Allocative Bias",
    "section": "5. Check for intersectional trends by studying the proportion of positive target labels broken out by your chosen group labels and an additional group label.",
    "text": "5. Check for intersectional trends by studying the proportion of positive target labels broken out by your chosen group labels and an additional group label.\n\nlabel1_groups_sex = df.groupby(['RACE', 'SEX'])['ESR_label'].mean()\nprint(label1_groups_sex)\n\nRACE  SEX\n1     1.0    0.489674\n      2.0    0.452264\n2     1.0    0.583333\n      2.0    0.200000\n3     1.0    0.265683\n      2.0    0.305195\n4     2.0    0.000000\n5     1.0    0.166667\n      2.0    0.230769\n6     1.0    0.500000\n      2.0    0.500000\n7     1.0    0.500000\n      2.0    0.500000\n8     1.0    0.250000\n      2.0    0.421053\n9     1.0    0.357143\n      2.0    0.359551\nName: ESR_label, dtype: float64\n\n\n\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nresult = label1_groups_sex.to_frame().reset_index()\nfigure = sns.barplot(x='RACE', y = 'ESR_label', hue = 'SEX', data = result)\nfigure.set(xlabel=\"Race\", ylabel=\"Education Status Recode\")\n\n[Text(0.5, 0, 'Race'), Text(0, 0.5, 'Education Status Recode')]"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#training-the-model",
    "href": "posts/Auditing Allocative Bias/index.html#training-the-model",
    "title": "Auditing Allocative Bias",
    "section": "Training the Model",
    "text": "Training the Model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])StandardScalerStandardScaler()LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#auditing-the-model",
    "href": "posts/Auditing Allocative Bias/index.html#auditing-the-model",
    "title": "Auditing Allocative Bias",
    "section": "Auditing the Model",
    "text": "Auditing the Model\n\nOverall Measures\n\nWhat is the overall accuracy of your model?\n\n\ny_hat = model.predict(X_test)\n(y_hat == y_test).mean()\n\n0.7514506769825918\n\n\nThe overall accuracy is 75.15%.\n\nWhat is the positive predictive value (PPV) of your model?\n\n\ndf_audit = pd.DataFrame(X_test, columns = features_to_use)\n\ndf_audit[\"RACE\"] = group_test\ndf_audit[\"ESR_label\"] = y_test\ndf_audit[\"predicted\"] = y_hat\ndf_audit[\"match\"] = (df_audit[\"predicted\"] == y_test)\ndf_audit\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RACE\n      ESR_label\n      predicted\n      match\n    \n  \n  \n    \n      0\n      49.0\n      19.0\n      5.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      2.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n      False\n      False\n    \n    \n      1\n      56.0\n      16.0\n      3.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n      True\n      False\n    \n    \n      2\n      11.0\n      8.0\n      5.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1.0\n      0.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      False\n      False\n      True\n    \n    \n      3\n      8.0\n      4.0\n      5.0\n      2.0\n      1.0\n      2.0\n      1.0\n      1.0\n      0.0\n      2.0\n      1.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1\n      False\n      False\n      True\n    \n    \n      4\n      18.0\n      1.0\n      5.0\n      17.0\n      2.0\n      0.0\n      1.0\n      1.0\n      3.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n      False\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2063\n      18.0\n      16.0\n      5.0\n      2.0\n      2.0\n      0.0\n      1.0\n      3.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n      True\n      True\n    \n    \n      2064\n      55.0\n      16.0\n      3.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n      True\n      False\n    \n    \n      2065\n      85.0\n      12.0\n      1.0\n      16.0\n      1.0\n      0.0\n      3.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1\n      False\n      False\n      True\n    \n    \n      2066\n      64.0\n      21.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      3\n      False\n      True\n      False\n    \n    \n      2067\n      50.0\n      22.0\n      4.0\n      0.0\n      2.0\n      0.0\n      1.0\n      3.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      False\n      True\n      False\n    \n  \n\n2068 rows × 19 columns\n\n\n\n\ncm = confusion_matrix(df_audit[\"ESR_label\"], df_audit[\"predicted\"])\nPPV = cm[1,1]/np.sum(cm[:,1])\nprint(PPV)\n\n0.7122232916265641\n\n\n\nWhat are the overall false negative and false positive rates (FNR and FPR) for your model?\n\n\nTP = cm[1,1] ## true positive\nFP = cm[0,1] ## false positive\nTN = cm[0,0] ## true negative\nFN = cm[1,0] ## false negative \n\nFNR = FN / (FN + TP)\nFPR = FP / (FP + TN)\n\nprint(\"False Positive Rate:\", FPR)\nprint(\"False Negative Rate:\", FNR)\n\nFalse Positive Rate: 0.26864330637915546\nFalse Negative Rate: 0.225130890052356\n\n\n\n\nBy-Group Measures\n\nWhat is the accuracy of your model on each subgroup?\n\n\naccuracy_sGroup = df_audit.groupby(['RACE'])['match'].mean()\nprint(accuracy_sGroup)\nprint(\" \")\nprint(\"Accuracy for white individuals:\", (y_hat == y_test)[group_test == 1].mean())\nprint(\"Accuracy for black individuals:\", (y_hat == y_test)[group_test == 2].mean())\n\nRACE\n1    0.755470\n2    0.875000\n3    0.677632\n4    1.000000\n5    1.000000\n6    0.500000\n8    0.727273\n9    0.823529\nName: match, dtype: float64\n \nAccuracy for white individuals: 0.7554704595185996\nAccuracy for black individuals: 0.875\n\n\n\nWhat is the PPV of your model on each subgroup?\n\n\nprint(\"Positive Predicitive Value for White Individuals:\", (np.sum(y_test[group_test == 1]) / np.sum(y_hat[group_test == 1])))\nprint(\"Positive Predicitive Value for Black Individuals:\", (np.sum(y_test[group_test == 2]) / np.sum(y_hat[group_test == 2])))\n\nPositive Predicitive Value for White Individuals: 0.9332627118644068\nPositive Predicitive Value for Black Individuals: 1.3333333333333333\n\n\n\n# RACE = 1, white individuals subgroup\n\ncm_W = confusion_matrix(y_test[group_test == 1], y_hat[group_test == 1])\n\nTP = cm_W[1,1] ## true positive\nFP = cm_W[0,1] ## false positive\nTN = cm_W[0,0] ## true negative\nFN = cm_W[1,0] ## false negative \n\nFNR = FN / (FN + TP)\nFPR = FP / (FP + TN)\nPPV = TP / (TP + FP)\n\nprint(\"PPV for White Individuals:\", PPV)\nprint(\"False Positive Rate for White Individuals:\", FPR)\nprint(\"False Negative Rate for White Individuals:\", FNR)\n\n# RACE = 2, black individuals subgroup\n\ncm_b = confusion_matrix(y_test[group_test == 2], y_hat[group_test == 2])\n\nTP = cm_b[1,1] ## true positive\nFP = cm_b[0,1] ## false positive\nTN = cm_b[0,0] ## true negative\nFN = cm_b[1,0] ## false negative \n\nFNR = FN / (FN + TP)\nFPR = FP / (FP + TN)\nPPV = TP / (TP + FP)\nprint(\" \")\nprint(\"PPV for Black Individuals:\", PPV)\nprint(\"False Positive Rate for Black Individuals:\", FPR)\nprint(\"False Negative Rate for Black Individuals:\", FNR)\n\nPPV for White Individuals: 0.7298728813559322\nFalse Positive Rate for White Individuals: 0.2692713833157339\nFalse Negative Rate for White Individuals: 0.21793416572077184\n \nPPV for Black Individuals: 1.0\nFalse Positive Rate for Black Individuals: 0.0\nFalse Negative Rate for Black Individuals: 0.25\n\n\n\n\nBias Measures\n\nIs your model approximately calibrated?\n\nOf our subgroups, the PPV for white individuals is around 0.73 and the PPV for white individuals is 1.0. Therefore, I would say that the model is approximately calibrated, despite the PPV for white individuals not being perfectly calibrated.\n\nDoes your model satisfy approximate error rate balance?\n\nMy model does not satisfy approximate error rate balance because the FPR and FNR are not equal across groups. For white individuals, the FPR and FNR are close, 0.269 and 0.217 respectively, however for black individuals, the FPR and FNR are very different, being 0.0 and 0.25 respectively. Therefore, my model does not satisfy approximate error rate balance.\n\nDoes your model satisfy statistical parity?\n\n\npositive_subgroups = df_audit.groupby(['RACE'])['predicted'].mean()\nprint(positive_subgroups)\n\nRACE\n1    0.516411\n2    0.375000\n3    0.407895\n4    0.000000\n5    0.142857\n6    0.400000\n8    0.545455\n9    0.372549\nName: predicted, dtype: float64\n\n\nTo check whether my model satisfies statistical parity, we can look at the proportion of individuals classified as employed, and check whether that proportion is the same across groups. As you can see, the majority of the values seem to be in a range of 0.37 to 0.54, however there are a few outliers (0.00 for race 4 and 0.143 for race 5.) Because of these outliers and a wider range, I would say that my model does not satisfy statistical parity."
  },
  {
    "objectID": "posts/Kernel Logistic Regression/index.html",
    "href": "posts/Kernel Logistic Regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "link to source code\nhttps://github.com/juliafairbank7/juliafairbank7.github.io/blob/main/posts/kernel-logistic-regression-post/KernelLogisticRegression.py\n\n\nOverview of Kernel Logistic Regression\nRegular logistic regression is a binary classification machine learning algorithm. One weakness of regular logistic regression is that the algorithm doesn’t work well with data that is not linearly separable. Kernel logistic regression is a technique that extends regular logistic regression to be compatible data that is not linearly separable.\nSimilar to other binary classification machine learning algorithms like perceptron and regular logistic regression, kernel logistic regression aims to predict the label of a data point based on some predictor variables.\nKernel logistic regression requires us to specify a kernel function and parameters for the kernel function. I’ll be using the radial basis function (RBF) kernel function. RBF takes in one parameter, gamma, which controls how “wiggly” the decision boundary should be. Larger gamma means a more wiggly decision boundary.\nThe algorithm uses a stochastic gradient descent to train the kernel logistic regression model, which is an iterative process. We can set the number of training iterations as well as the learning rate to control how much the parameters change on each update.\nOnce the kernel logistic regression model is fully trained, there will be an alpha value for each data point and one bias value.\n\n\nGenerating Non-Linearly Separable Dataset\nBecause kernel logisitic regression is a technique that extends regular logistic regression to be compatible data that is not linearly separable, let’s generate some non-linearly separable data!\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(50, shuffle = True, noise = 0.2)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\nFrom this dataset, you can start to imagine what the non-linear classification/division might look like. Let’s see how the kernel logisitic regression algorithm attempts to classify that separation.\nBut first, we’re going to use a pre-implemented version of logistic regression and a visualization tool, so let’s import that.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\nLR = LogisticRegression()\nLR.fit(X, y)\n\nplot_decision_regions(X, y, clf = LR)\ntitle = plt.gca().set(title = f\"Accuracy = {(LR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nThis graphs shows a linear regression classification of non-linearly separable data, which cannot perfectly separate non-linearly separable data. Now let’s try with kernel logisitic regression.\n\n\nApplying Kernel Logistic Regression\n\nfrom KernelLogisticRegression import KernelLogisticRegression\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {(KLR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs you can see, the kernel logisitic regression creates organic, non-linear separations in an attempt to better classify the data. In comparing the accuracy of kernel logisitic regression to linear logistic regression, our accuracy has improved, making this a better classifier for non-linearly separable data.\n\n\nOverview of the KLR Functions\n\n\nfit()\nKernelLogisticRegression.fit(X, y) is the main function that learns the optimal parameters \\({v}\\). To do this, the function first computes a kernel matrix of X with itself, which is used to minimize the empiricial risk and saved as \\({v}\\).\n\n\npredict()\nKernelLogisticRegression.predict(X) takes the dot product of the kernel matrix of X with the value \\({v}\\) from the fit() function, then turns dot product value into a binary value.\n\n\nscore()\nKernelLogisticRegression.score(X, y) uses the predict() function to make a prediction matrix of \\({X}\\) and compute the accuracy.\nNow, let’s experiment with this algorithm. Because we are using RBF, let’s try changing the value of gamma. As a reminder, a larger gamma means a more wiggly decision boundary.\n\n\nExperimenting with Larger Gamma Values\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs you can see, the classifier draws an orange blob around the orange data points rather than drawing a line. This is sufficient to achieve 100% accuracy on the training data.\nHowever, if we were to generate new data, we will see a lower accuracy as the classifier blog remains the same, depiste new data points being generated in different locations.\n\n\nGenerating a New Dataset\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs you can see, the classifier from the first dataset remains the same, despite the new dataset and feature positions. This results in a lower accuracy score since this classifier isn’t made uniquely to fit this dataset.\nNext, let’s try varying the noise, which is a parameter when we call make_moons. The noise determines how spread out the two crescents of points are. Let’s try changing the amount of noise to see the relationship between noise and gamma.\n\n\nExperimenting with Noise\n\n\nNoise = 0\n\nX_lowNoise, y_lowNoise = make_moons(50, shuffle = True, noise = 0)\nplt.scatter(X_lowNoise[:,0], X_lowNoise[:,1], c = y_lowNoise)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nplt.title(\"Testing Dataset, Noise = 0\")\n\nText(0.5, 1.0, 'Testing Dataset, Noise = 0')\n\n\n\n\n\n\nKLR_lowG = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR_lowG.fit(X_lowNoise, y_lowNoise)\nplot_decision_regions(X_lowNoise, y_lowNoise, clf = KLR_lowG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_lowG.score(X_lowNoise, y_lowNoise)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Noise = 0, Gamma = 0.1', fontsize=14, y=1)\n\nText(0.5, 1, 'Noise = 0, Gamma = 0.1')\n\n\n\n\n\n\nKLR_highG = KernelLogisticRegression(rbf_kernel, gamma = 100)\nKLR_highG.fit(X_lowNoise, y_lowNoise)\nplot_decision_regions(X_lowNoise, y_lowNoise, clf = KLR_highG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_highG.score(X_lowNoise, y_lowNoise)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Noise = 0, Gamma = 100', fontsize=14, y=1)\n\nText(0.5, 1, 'Noise = 0, Gamma = 100')\n\n\n\n\n\nWith no noise, a low gamma value (0.1) has a perfect accuracy and increasing it has no affect.\n\n\nNoise = 1.0\n\nX_highNoise, y_highNoise = make_moons(50, shuffle = True, noise = 1.0)\nplt.scatter(X_highNoise[:,0], X_highNoise[:,1], c = y_highNoise)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nplt.title(\"Testing Dataset, Noise = 1.0\")\n\nText(0.5, 1.0, 'Testing Dataset, Noise = 1.0')\n\n\n\n\n\n\nKLR_lowG = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR_lowG.fit(X_highNoise, y_highNoise)\nplot_decision_regions(X_highNoise, y_highNoise, clf = KLR_lowG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_lowG.score(X_highNoise, y_highNoise)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Noise = 1.0, Gamma = 0.1', fontsize=14, y=1)\n\nText(0.5, 1, 'Noise = 1.0, Gamma = 0.1')\n\n\n\n\n\n\nKLR_highG = KernelLogisticRegression(rbf_kernel, gamma = 100)\nKLR_highG.fit(X_highNoise, y_highNoise)\nplot_decision_regions(X_highNoise, y_highNoise, clf = KLR_highG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_highG.score(X_highNoise, y_highNoise)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Noise = 1.0, Gamma = 0.1', fontsize=14, y=1)\n\nText(0.5, 1, 'Noise = 1.0, Gamma = 0.1')\n\n\n\n\n\nWhen we increase noise at 0.5, a low gamma value (0.1) yields a lower accuracy that improves as gamma increases to a higher value.\n\n\nConcentric Circles Dataset\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.8, noise=.05)\n\nfig_c = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nKLR_c_lowG = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR_c_lowG.fit(X_c, y_c)\nplot_decision_regions(X_c, y_c, clf = KLR_c_lowG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_c_lowG.score(X_c, y_c)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Gamma = 0.1', fontsize=14, y=1)\n\nText(0.5, 1, 'Gamma = 0.1')\n\n\n\n\n\n\nKLR_c_highG = KernelLogisticRegression(rbf_kernel, gamma = 100)\nKLR_c_highG.fit(X_c, y_c)\nplot_decision_regions(X_c, y_c, clf = KLR_c_highG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_c_highG.score(X_c, y_c)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Gamma = 100', fontsize=14, y=1)\n\nText(0.5, 1, 'Gamma = 100')\n\n\n\n\n\nFor concentric circle datasets where the features are closely concentric, a higher gamma value doesn’t always improve the accuracy of the classification. KLR with low gamma values still yield high accuracies that don’t necessarily improve by increasing the gamma."
  },
  {
    "objectID": "posts/Logistic Regression/index.html",
    "href": "posts/Logistic Regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "link to source code\nhttps://github.com/juliafairbank7/juliafairbank7.github.io/blob/main/posts/logistic-regression-post/LogisticRegression.py\n\n\nOptimization of Logistic Regression\nLogistic regression is a classifier that estimates the probability of something based on a given dataset of independent variables.\nIn this blog post, I will implement a simple gradient descent and a stochastic gradient descent and will compare their performance for training logistic regression.\n\n\nApplying Logistic Regression\nLet’s start by generating a dataset that we will use on our gradient descent and our stochastic gradient descent.\n\nfrom LogisticRegression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.seterr(all='ignore') \n\nnp.random.seed(777)\n\nn = 100\np_features = 3\n\nX1, y1 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\n\nGradient Descent\nWhen we call LR.fit(X, y), LR should have an instance variable of weights called w, which is a vector of weights, including the bias term. LR should have an instance variable called LR.loss_history which is a list of the evolution of the loss over the training period, and an instance variable called LR.score_history, which is a list of the evolution of the score over the training period.\nLet’s see how this works on our dataset.\n\nnp.random.seed(777)\n\nn = 100\np_features = 3\n\nX1, y1 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\nLR_f = LogisticRegression()\nLR_f.fit(X1, y1, alpha = 0.1, max_epochs = 1000)\n\nfig = plt.scatter(X1[:,0], X1[:,1], c = y1)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color = \"black\")\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nfig1 = draw_line(LR_f.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Gradient Descent\")\n\nText(0.5, 1.0, 'Gradient Descent')\n\n\n\n\n\nAs you can see, our simple gradient descent has drawn its classifying line to separate the two features. It’s not a perfect classifier, but let’s see what the loss and score values look like.\n\nprint(LR_f.loss_history[-3])\nprint(LR_f.score_history[-1])\n\n0.02319692785879772\n0.99\n\n\nWe have a low loss and high score, meaning this line is a good strong separating line for logisitic regression.\nNow, let’s compare this with the stochastic gradient descent.\n\n\nStochastic Gradient Descent\nLogisticRegression.fit_stochastic(X, y) is an alternative version of the fit() method which computes a stochastic gradient by picking a random subset, computing the stochastic gradient, performing an update, then repeating. When LR.fit_stochastic(X, y) is called, LR should have an instance variable of weights called w, which is a vector of weights, including the bias term b. LR should have an instance variable called LR.loss_history which is a list of the evolution of the loss over the training period, and an instance variable called LR.score_history, which is a list of the evolution of the score over the training period.\n\n\nTesting Stochastic\nAgain, let’s start by re-generating that same random dataset.\n\nnp.random.seed(777)\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\nLR_s = LogisticRegression()\nLR_s.fit_stochastic(X2, y2, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color = \"black\")\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig2 = draw_line(LR_s.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Stochastic Gradient Descent')\n\n\n\n\n\nAs you can see, stochastic gradient descent has drawn its line of best fit to separate the two features. This line seems to separate the data better than the simple gradient, so let’s look at the score and loss values to compare more accurately. Since stochastic gradient descent, we’ll look at three of the loss values to see how it changes over iterations.\n\nprint(LR_s.loss_history[-3])\nprint(LR_s.loss_history[-2])\nprint(LR_s.loss_history[-1])\nprint(LR_s.score_history[-1])\n\n0.014117604281679486\n0.014115400515806372\n0.014113204314421224\n1.0\n\n\nAgain, we have very low loss values that get smaller over iterations, and a perfect score, making the stochastic gradient descent a great classifier that had better results classifying this dataset than the simple gradient descent.\nFor sake of visualization and comparision, let’s plot the two fit methods on the same graph.\n\n\nComparison Plotting\n\nnp.random.seed(777)\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\nLR_f_b = LogisticRegression() #red\nLR_f_b.fit(X3, y3, alpha = 0.1, max_epochs = 1000)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color = \"red\", label = 'gradient')\n\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nfig3 = draw_line(LR_f_b.w_hat, -2, 2)\n\n\nLR_s_b = LogisticRegression() #blue\nLR_s_b.fit_stochastic(X3, y3, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"blue\", label='stochastic')\n\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nfig3 = draw_line(LR_s.w_hat, -2, 2)\nplt.title(\"Comparing Gradient Descent with Stochastic Gradient Descent\")\n\nlegend = plt.legend() \n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X3, y3, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, color = \"blue\", label = \"stochastic gradient; alpha = 0.05\")\n\nLR.fit_stochastic(X3, y3, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, color = \"teal\", label = \"stochastic gradient; alpha = 0.1\")\n\nLR.fit(X3, y3, alpha = .05, max_epochs = 10000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, color = \"red\", label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \nplt.title(\"Comparing Loss History Across Fit Methods\")\n\nText(0.5, 1.0, 'Comparing Loss History Across Fit Methods')\n\n\n\n\n\nFrom this graph, we can see that the stochastic method converges faster than gradient descent with a steeper decline. The gradient method takes longer and requires more epochs to find a good solution.\n\n\nExperimenting\nLet’s try experimenting with increasing our alpha value.\n\nnp.random.seed(777)\n\nX4, y4 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig4 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\n#large alpha value\nLR_f_a_large = LogisticRegression()\nLR_f_a_large.fit(X4, y4, alpha = 100, max_epochs = 10000)\n\n#normal alpha value\nLR_f_a_norm = LogisticRegression()\nLR_f_a_norm.fit(X4, y4, alpha = .1, max_epochs = 10000)\n\nfig4 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"maroon\", label='large alpha, a = 100')\n\n#alpha = 200\nfig4 = draw_line(LR_f_a_large.w_hat, -2, 2)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"rosybrown\", label='normal alpha, a = 0.1')\n    \n#alpha = 0.01\nfig4 = draw_line(LR_f_a_norm.w_hat, -2, 2)\n\nlegend = plt.legend() \nplt.title(\"Experimenting with Large Alpha Value\")\n\n\nText(0.5, 1.0, 'Experimenting with Large Alpha Value')\n\n\n\n\n\n\nxlab = plt.xlabel(\"Number of Iterations\")\nylab = plt.ylabel(\"Loss History Score\")\n\nnum_steps1 = len(LR_f_a_large.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR_f_a_large.loss_history, color = \"maroon\", label = 'large alpha, a = 100')\n\nnum_steps2 = len(LR_f_a_norm.loss_history)\nplt.plot(np.arange(num_steps2) + 1, LR_f_a_norm.loss_history, color = \"rosybrown\", label = 'normal alpha, a = 0.1')\n\nlegend = plt.legend() \n\n\n\n\n\n\nExperimenting with Non-Linearly Separable Data\nWe’ve seen how our simple gradient descent and stochtastic gradient descent work on linearly separable data, but let’s see how they compare on non-linearly separable data.\nFirst, let’s generate some non-linearly separable data.\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.3, noise=.05)\n\nfig_c = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR_f_NLS_a_large = LogisticRegression()\nLR_f_NLS_a_large.fit(X_c, y_c, alpha = 100, max_epochs = 10000)\n\nLR_f_NLS_a_norm = LogisticRegression()\nLR_f_NLS_a_norm.fit(X_c, y_c, alpha = 0.1, max_epochs = 10000)\n\nfig_c = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"maroon\", label='large alpha, a = 100')\n\nfig_c = draw_line(LR_f_NLS_a_large.w_hat, -2, 2)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"rosybrown\", label='normal alpha, a = 0.1')\n\nfig_c = draw_line(LR_f_NLS_a_norm.w_hat, -2, 2)\n\n\n\n\n\nxlab = plt.xlabel(\"Number of Iterations\")\nylab = plt.ylabel(\"Loss History Score\")\n\nnum_steps1 = len(LR_f_NLS_a_large.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR_f_NLS_a_large.loss_history, color = \"maroon\", label = 'large alpha, a = 100')\n\nnum_steps2 = len(LR_f_NLS_a_norm.loss_history)\nplt.plot(np.arange(num_steps2) + 1, LR_f_NLS_a_norm.loss_history, color = \"rosybrown\", label = 'normal alpha, a = 0.1')\n\nlegend = plt.legend() \n\n\n\n\nAs you can see, the large alpha doesn’t appear to work in the way it should (converges before it begins), while the normal alpha seems to separate the features with a low-ish loss score.\n\n\nExperimenting with Batch Size\n\nnp.random.seed(777)\n\nX5, y5 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig5 = plt.scatter(X5[:,0], X5[:,1], c = y5)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\nLR_smallBatch = LogisticRegression()\nLR_smallBatch.fit_stochastic(X5, y5, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .1) \n\nnum_steps = len(LR_smallBatch.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_smallBatch.loss_history, color = \"slategray\", label = \"small batch size, batch_size = 5\")\n\nLR_largeBatch = LogisticRegression()\nLR_largeBatch.fit_stochastic(X5, y5, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 50, \n                  alpha = .1) \n\nnum_steps = len(LR_largeBatch.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_largeBatch.loss_history, color = \"navy\", label = \"large batch size, batch_size = 50\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of Iterations\")\nylab = plt.ylabel(\"Loss History Score\")\nplt.title(\"Comparing Batch Size on Stochastic Gradient Descent\")\n\nlegend = plt.legend() \n\n\n\n\nIn this experiment, you can see that the larger batch size converges at a faster rate than the small batch size. However, they both appear converge at the same amount of epochs."
  },
  {
    "objectID": "posts/Learning from Timnit Gebru/index.html",
    "href": "posts/Learning from Timnit Gebru/index.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Timnit Gebru is a well-known computer scientist and AI researcher, with the majority of her work focusing on bias and equity. Gebru received her bachelor’s degree in computer engineering from the University of Addis Ababa, her master’s degree in electrical engineering from Stanford University, and a PhD in computer vision from the University of Washington.\nGebru has worked at various tech companies, including Apple, Microsoft, and Google, where she worked on creating space for underrepresented groups in AI. In 2020, after a company dispute over a research paper identifying the risks of large language models, Gebru was fired from her position as a co-lead of Google’s Ethical AI team. Her removal from Google sparked widespread debate about the inherent sexism and racism in the tech industry and highlighted the importance of considering the role of ethics in AI development.\nMiddlebury College is lucky to be joined by Timnit Gebru on Monday, April 24th for a virtual Zoom conversation discussing bias and social impacts of artificial intelligence."
  },
  {
    "objectID": "posts/Learning from Timnit Gebru/index.html#discussion-reflection",
    "href": "posts/Learning from Timnit Gebru/index.html#discussion-reflection",
    "title": "Learning from Timnit Gebru",
    "section": "Discussion Reflection",
    "text": "Discussion Reflection\nDr. Gebru began by discussing her foundation, DAIR, and their mission of both mitigating the harms of current AI systems along with imagining & executing their own, different technological future. She used this mission statement to argue the big tech leaders are exclusively pouring their resources and time into problems that they are interested in, rather than problems the world needs addressing. She used Karen Hao’s tweets to argue that technology, among basic human resouces, has never been distributed equally, so not only is technology being developed in line with the interest of some of the largest companies in the world, but the technology that is developed will still only be reachable by those with an abudance of resources. She uses the tweets of some big technology executives to contrast this perspective and highlight how these leaders claim this technology changes the world and has the potential to create an unimaginable uptopia and condemns this perspective, arguing that this is not a utopia that is wanted by or benefits all.\nGebru also discusses eugenics and the relationship it has to artifical general intelligence. She analyzes the term AGI deeply, trying to figure out exactly what it is, concluding that there isn’t a single, clear, definite definition. She continues on to give background on first and second wave eugenics, then goes through a few different philosophies like effective altruism and transhumanism, condemning them all. Dr. Gebru concludes the presentation with discussing the AGI apocapolse and the biases the AGI has the ability to perpetutate unless actively fought against.\nI personally had a very tough time following this presentation. I had to watch the recording a few different times to understand the logic of her arguments on account of how many different topics were brought up out of context. I thought this presentation was very extreme and was disappointed that her argument was rooted in emotion and political opinions, using tweets as the main source of data, rather than data or logical reasoning. I believe there is a lot to be said about the future of AI and the ethics behind it (I did my senior philosophy thesis on it) and was incredibly shocked to find that this presentation was more of an attack on the individuals of the side rather than an explanation of her research and beliefs. I was especially disappointed with her response to the first question, which contributed to my opinion that she substainially lacked any clear defense of her argument in this presentation, instead building her talking points on the tweets of her opponents. Dr. Gebru has a very fascinating background and impressive experience and I wish she had relied more on her research and education to build and support her argument."
  },
  {
    "objectID": "posts/Learning from Timnit Gebru/index.html#reflect-on-the-process",
    "href": "posts/Learning from Timnit Gebru/index.html#reflect-on-the-process",
    "title": "Learning from Timnit Gebru",
    "section": "Reflect on the Process",
    "text": "Reflect on the Process\nI had a challenging time with this interaction. I think Dr. Gebru’s work is incredibly interesting and important to the current state of our world, however, I think Dr. Gebru failed to capture her audience and explain her work in a reliable way. I’m not sure how my thoughts on this discussion contribute to my feelings and opinions toward these issues as a whole but I know that I’m not satisified with the discussion or Gebru’s interactions with undergraduate students."
  },
  {
    "objectID": "posts/Linear Regression/index.html",
    "href": "posts/Linear Regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "link to source code\nhttps://github.com/juliafairbank7/juliafairbank7.github.io/tree/main/posts/linear-regression-post\n\n\nOverview of Linear Regression\nWith data that has a linear relationship between two variables, linear regression allows you to find the best line that fits that relationship. The Least Squares Regression Line is the line that minimizes the variance (you can think about it like vertical height) between the data points to the regression line. This best line of fit minimizes the (It’s called a “least squares” because the variance represents the sum of squares of the errors).\nLeast-squares linear regression is a convex linear model that makes predictions of the form \\(\\tilde{y}_i = \\langle w, {x}_{i} \\rangle\\).\nThe loss function that we will be using is \\(l(\\tilde{y}, y) = (\\tilde{y} - y)^{2}\\), which says that the loss is equal to the squared error (least squares!)\nOur empirical loss minimization function is\n$ = $ arg min \\(L(w)\\)\nThere are multiple ways to solve this empirical loss minimization function, and we will be focusing on two of them: analytically and using gradient descent.\nSo, we will be implementing least-squares linear regression in these two different ways: 1. Analytical Fit 2. Gradient Descent Fit\n\n\nAnalytical Fit\nFor the analytical implementation of least-squares linear regression, we can use an explicit formula that uses matrix inversion. This formula, \\(\\hat{w} = {X}^{T}{X}^{-1}X^{T}y\\), will calculate our optimal weight vector.\n\n\nGradient Descent Fit\nFor the gradient descent implementation of least-squares linear regression, we will need to compute the gradient with respect to \\(\\hat{w}\\) and repeat until convergence. To do this without killing our computer by over-computing, we can calculate \\(P = {X}^{T}{X}\\) and \\(q = {X}^{T}y\\) just once. From there, our gradient is \\(2(Pw-q)\\).\nBy calculating our gradient this way, we are eliminating the chance for the run-time to shoot up depending on the number of data points.\n\n\nDemo Linear Regression on Data\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom LinearRegression import LinearRegression\n\nLinReg = LinearRegression()\n\nLet’s take a look at how our analytical fit linear regression works on some sample data.\n\n\nAnalytical Fit Demo\n\nw0 = -0.5\nw1 =  0.7\n\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1) \n\n\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nplt.scatter(x, y)\n\nLinReg.fit_analytic(x, y)\n\nx_fake = np.linspace(0, 1, 101)[:,np.newaxis]\n\npredictions = LinReg.predict(x_fake)\n\nplt.plot(x_fake, predictions, color = \"black\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\nAs you can seen through the graph above, the analytical fit has classified the least squares regression line through our data points. Now let’s see how our other method works.\n\n\nGradient Descent Fit Demo\n\nw0 = -0.5\nw1 =  0.7\n\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1) \n\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nplt.scatter(x, y)\n\nLinReg.fit_gradient(x, y)\n\nx_fake = np.linspace(0, 1, 101)[:,np.newaxis]\n\npredictions = LinReg.predict(x_fake)\nprint(predictions.shape)\n\nplt.plot(x_fake, predictions, color = \"blue\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n(101, 1)\n\n\n\n\n\nHere, our gradient descent method draws its least squares regression line through our data points. Let’s see how our score value has changed over our epochs.\n\n\nScore History of Gradient Descent Fit Score over Iterations\n\nplt.plot(LinReg.score_history, color = \"blue\");\n\n\n\n\n\n\nCreate Testing & Validation Data\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow, let’s use this testing and validation data to see our testing and validation scores from our analytical fit.\n\nfrom LinearRegression import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train)\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = -142.5592\nValidation score = -145.883\n\n\nWe can also see our training and validation scores from our gradient descent fit.\n\nLR2 = LinearRegression()\nLR2.fit_gradient(X_train, y_train, alpha = 0.00003, max_epochs = 2000)\n\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.3969\nValidation score = 0.3531\n\n\nLet’s see how our gradient descent score has changed over our iterations (epochs).\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(title = \"Score History for Gradient Descent Fit\", xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\nExperimenting with the Number of Features Used\nNow that we’ve seen how linear regression works on 1 feature and a bias term, let’s try to increase the number of features to 5.\n\ndef LR_analytic_features(n_train = 100, n_val = 100, p_features = 5, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nn_train = 1000\nn_val = 100\nnoise = 0.2\n\nfeature_count = []\ntesting_scores = []\nvalidation_scores = []\n    \nfor i in range(n_val-1):\n    p_features = i\n    feature_count.append(i)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR_analytic_features = LinearRegression()\n    LR_analytic_features.fit_analytic(X_train, y_train)\n    testing_scores.append(LR_analytic_features.score(X_train, y_train).round(4))\n    validation_scores.append(LR_analytic_features.score(X_val, y_val).round(4))\n\nplt.plot(testing_scores)\nplt.plot(validation_scores)\nlabels = plt.gca().set(title = \"Increasing Features on Analytic Fit\", xlabel = \"Features\", ylabel = \"Score\")\n\n\n\n\n\ndef LR_GD_features(n_train = 100, n_val = 100, p_features = 5, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nn_train = 1000\nn_val = 100\nnoise = 0.2\n\nfeature_count = []\ntesting_scores = []\nvalidation_scores = []\n    \nfor i in range(n_val-1):\n    p_features = i\n    feature_count.append(i)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR_GD_features = LinearRegression()\n    LR_GD_features.fit_gradient(X_train, y_train, alpha = 0.00003, max_epochs = 40000)\n    testing_scores.append(LR_GD_features.score(X_train, y_train).round(4))\n    validation_scores.append(LR_GD_features.score(X_val, y_val).round(4))\n\nKeyboardInterrupt: \n\n\n\nplt.plot(validation_scores)\nplt.plot(testing_scores)\nlabels = plt.gca().set(title = \"Increasing Features on Gradient Descent Fit\", xlabel = \"Features\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/Perceptron/index.html",
    "href": "posts/Perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Overview of Perceptron Model\nThe Peceptron Algorithm is a binary, linear classification machine learning algorithm. The algorithms aims to find a rule that separates two distinct groups in some data.\nThe algorithm takes in a vector of n data points that have k features as input and predicts a label by calculating the weighted sum of the inputs and a bias b, and predicts 1 if positive and 0 if negative.\nThe perceptron algorithm aims to find a good (not always the best) \\(\\tilde{w}\\) using the following algorithm:\n\nBegin with some random \\(\\tilde{w}^{(0)}\\)\n“Until we’re done” in some time-step t:\n\nPick a random data point i within the n data points\nCompute \\(\\hat{y}_{i}^{(t)} = \\langle \\tilde{w}^{(t)}, \\tilde{x}_{i} \\rangle\\)\nIf \\(\\hat{y}_{i}^{(t)}{y}_{i}\\) > 0, then point is correctly classified – pass!\nIf \\(\\hat{y}_{i}^{(t)}{y}_{i}\\) < 0, then perform the update:\n\n\\[\n     \\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + {y}_i \\tilde{w}_{i}\n    \\]\n\nIn order to actually separate the two classes, the algorithm uses the prediction function which follows the rule: $ ^{(1)}, _{(i)} _i = 1 $,\nwhich says that if the dot product between \\(\\tilde{w}^{(1)}\\) and \\(\\tilde{w}_{(i)}\\) is less than 0, label it 1, otherwise label it -1.\nLet’s see how my perceptron algorithm works on a set of data.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.gca().set(title = \"Data Points\")\n\n\n\n\n\n\nApplying the fit() method\nPerceptron.fit(X, y) is my primary method. When p.fit(X, y) is called, p should have an instance variable of weights called w. This w is the vector in the classifier above. Additionally, p should have an instance variable called p.history which is a list of the evolution of the score over the training period.\nWithin the fit() function, we perform the perceptron update that correspondes to Equation 1, which states:\n\\[\n\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb1 (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{w}_{(i)} \\rangle \\lt 0)  \\tilde{y}_i \\tilde{x}_i\n\\]\n\n\napplying the predict() method\nPerceptron.predict(X) returns a vector of predicted labels on the data using the function \\[  \\langle \\tilde{w}^{(1)}, \\tilde{w}_{(i)} \\rangle \\lt 0 \\iff {y}_i = 1\n\\]\nSee the linear classification below that separates the two classes.\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#p.score(X, y)\n\n\n\n\n\n\napplying the score() method\nPerceptron.score(X, y) returns the accuracy of the perceptron as a number between 0 and 1, with 1 corresponding to perfect classification. We can see how the algorithm updates through different accuracies/losses, then ultimately converges to 0 with a perfect classification. The algorithm converges at a loss of 0 when it has successfully located a hyperplane that perfectly separates the two classes.\n\n# import Perceptron from source.py\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nHowever, this algorithm only converges when the data is lineraly separable, meaning there exists some line that separates the two classes. So let’s take a look at a nonlineraly separable dataset to see how the algorithm responds.\n\n\nnon-lineraly separable data\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.3, noise=.05)\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs you can see from this datset, there is no straight line that could separate the two classes, making this a nonlinearly separable dataset. Let’s see how the perceptron algorithm fits this data and where it draws the hyperplane.\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs you can see, the line drawn by the algorithm does not perfectly separate the two classes. Let’s look at the loss history graph to see the different updates.\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nAs you can see in the graph above, the algorithm never converges to 0, proving that the algorithm does not work on non-linearly separable datasets.\nNext, let’s see how the algorithm handles datasets with higher dimensions.\n\n\n5-Dimension Data\n\nnp.random.seed(12345)\n\nn = 100\np_features = 5\n\nX_5, y_5 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -3, 4, 1), (1.7, 1.7, 1, 53, 5)])\n\np = Perceptron()\np.fit(X_5, y_5, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nAs you can see in this loss history graph, the algorithm converges to 0 around iteration 65. Because the algorithm eventually has a loss of 0, this means there exists a hyperplane that perfectly separates the two classes. So, the perceptron algorithm works on multi-dimensional data, as long as it is lineraly separable.\n\n\nrun-time complexity()\nFrom Equation 1,\n\\[\n\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb1 (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{w}_{(i)} \\rangle \\lt 0)  \\tilde{y}_i \\tilde{x}_i\n\\]\nthe run-time complexity of one iteration would be \\(O({p})\\), because the dot product of \\(\\langle \\tilde{w}^{(t)}, \\tilde{w}_{(i)} \\rangle\\) takes \\(O({p})\\) time, the dot product of \\(\\tilde{y}_i \\tilde{x}_i\\) takes \\(O({p})\\) time, and \\(\\tilde{w}^{(t)}\\) takes \\(O({p})\\) time, for a total of \\(O({3p})\\) time, which simplifies to \\(O({p})\\).\nFrom this , we know that the runtime complexity depends only on the number of features, \\({p}\\), and doesn’t depend on the number of datapoints, \\({n}\\)."
  },
  {
    "objectID": "posts/Final Project/LinearRegressionRFE.html",
    "href": "posts/Final Project/LinearRegressionRFE.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "Source code\nhttps://github.com/CeceZiegler1/ML_Final_Proj/blob/main/LinearRegressionAnalytic.py\nBelow, we fit our model on the x_train and and y_train datasets, and then print out the training and validation scores. This model is fitted on all 60 features in the dataset. We can see from the scores, that it is not performing great, as a validation score below 50% indicates we could do better by just randomly selecting. We are going to perform a recursive feature elimination that we also implemented in our source code to see if we can find the optimal number of features to use to obtain the best score.\n\nfrom LinearRegressionAnalytic import LinearRegressionAnalytic\nfrom LinearRegressionAnalytic import rfe\n\n#Seeing how the model performs without RFE\n\nLR = LinearRegressionAnalytic()\nLR.fit(x_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(x_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(x_test, y_test).round(4)}\")\n\nTraining score = 0.5258\nValidation score = 0.4605\n\n\nBelow, we create an array to store the score that is produced with each different number of features used in the model as selected by our RFE. We use a for loop to loop through each value from 1-60 and display the score at each iteration in a graph. What we see from the graph, is even with using fewer features, our score never gets above around 45%. The best scores come around 12-15 features and 55-60 feautres. Even still, the scores at these points aren’t very good. Although we were hoping linear regression would perform well on our dataset, it doesn’t appear to be the case. Because of this, we are going to implement a random forest tree to see if we can obtain a better validation score on our dataset.\n\n\n# compute the score for each value of k\nval_scores = []\ntrain_scores = []\nfor k in range(60):\n    selected_features = rfe(x_train, y_train, k)\n    feature_mask = np.zeros(x_train.shape[1], dtype=bool)\n    #masking to include only the selected features\n    feature_mask[selected_features] = True\n    #subseting x train and test to include only selected feautres\n    X_train_selected = x_train.loc[:, feature_mask]\n    X_test_selected = x_test.loc[:, feature_mask]\n    lr = LinearRegressionAnalytic()\n    #fitting model on selected features\n    lr.fit(X_train_selected, y_train)\n    #appending score to score list\n    val_scores.append(lr.score(X_test_selected, y_test))\n    train_scores.append(lr.score(X_train_selected, y_train))\n\n# plot the results\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 61), val_scores, label='Testing accuracy')\nplt.plot(range(1, 61), train_scores, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\n\n[33, 34, 3, 35, 37, 6, 39, 2, 43, 45, 14, 19, 23, 28]\n\n\nBelow, we will show the 13 most important features as obtained through our rfe.\n\ndata.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\n\n\n\n\n  \n    \n      \n      max_rear_shoulder_x\n      max_rear_shoulder_y\n      range_lead_hip_z\n      max_rear_shoulder_z\n      max_torso_y\n      range_lead_shoulder_z\n      max_torso_pelvis_x\n      min_rfx\n      min_rfy\n      range_rear_shoulder_y\n      range_torso_pelvis_x\n      max_lead_hip_z\n      max_pelvis_y\n    \n  \n  \n    \n      0\n      550.0243\n      514.1198\n      778.1339\n      1188.6807\n      182.7302\n      867.6362\n      182.4743\n      -232.2776\n      -88.0097\n      689.2249\n      233.0842\n      384.3450\n      88.3858\n    \n    \n      1\n      638.6019\n      535.2822\n      960.4793\n      1278.5380\n      196.9712\n      1054.5098\n      236.0902\n      -189.7241\n      -106.2254\n      812.9988\n      306.7874\n      520.8627\n      106.4238\n    \n    \n      2\n      580.0406\n      472.9189\n      784.0413\n      1588.7207\n      248.4432\n      988.9415\n      222.8233\n      -124.4299\n      -84.5785\n      708.1030\n      313.2967\n      433.6955\n      82.5397\n    \n    \n      3\n      635.8561\n      484.2663\n      1036.2757\n      888.1270\n      166.9048\n      1472.7250\n      168.7606\n      -175.8547\n      -122.1629\n      732.5588\n      228.6738\n      489.4716\n      81.4764\n    \n    \n      4\n      566.9714\n      502.2202\n      1093.3019\n      1487.6143\n      191.2448\n      1130.6572\n      220.7400\n      -219.5387\n      -72.5831\n      699.1772\n      286.4758\n      597.7220\n      75.9968\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      631.5529\n      488.3580\n      980.3030\n      1575.2948\n      165.8830\n      1150.6032\n      147.9856\n      -114.1301\n      -173.2356\n      804.6660\n      239.9022\n      354.5130\n      124.7927\n    \n    \n      633\n      571.2316\n      477.7701\n      748.3298\n      1604.9299\n      145.5400\n      1026.0944\n      188.9410\n      -113.4915\n      -157.5923\n      735.1128\n      276.8293\n      324.9995\n      137.1521\n    \n    \n      634\n      549.3600\n      407.3251\n      526.3367\n      1393.4961\n      128.0184\n      1029.3547\n      257.2261\n      -112.7565\n      -111.9854\n      584.3304\n      348.2130\n      207.2101\n      128.8111\n    \n    \n      635\n      623.2650\n      463.8467\n      1248.0062\n      1715.0544\n      136.8013\n      892.8699\n      177.4202\n      -122.3425\n      -161.2802\n      725.1355\n      266.6244\n      282.0038\n      157.1024\n    \n    \n      636\n      599.2501\n      505.9937\n      1433.3273\n      1480.4099\n      143.7898\n      1233.8176\n      169.0549\n      -165.5618\n      -132.0637\n      700.1916\n      234.5590\n      500.9032\n      107.8579\n    \n  \n\n637 rows × 13 columns\n\n\n\n\nx_train = x_train.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\nx_test = x_test.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\nlr.fit(x_train, y_train)\nlr.score(x_test, y_test)\n\n0.386607442390802"
  },
  {
    "objectID": "posts/Final Project/RFE.html",
    "href": "posts/Final Project/RFE.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "X = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n#Set up\n\n\nmodel = LinearRegression()\nfeat_select = RFE(model, n_features_to_select = 10, step = 1)\n\nWhen using Recursive Feature Elimination, we need to select a few variables. First, we need to select what model we want to use. Since we are using multiple linear regression, we select linear regression. We also need to select how many features we want to use, and how many we want to remove during each recursion. These we chose arbitrarily for this example to be 10 and 1 respectively.\nIf we then run our RFE, we find that it classified our features as True or False, where True refers to features it selected and False ones it did not.\n\nfeat_select.fit(X,y)\nfeat_select.support_\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True,  True, False, False, False, False,  True,  True,  True,\n       False,  True,  True, False,  True])\n\n\nFrom this we can see that it selected features :42, 43, 46, 47, 52, 53, 54, 56, 57 and 59. This means that the data set we would train our model on is the following\n\ndata.iloc[:,[42,43,46,47,52,53,54,56,57,59]]\n\n\n\n\n\n  \n    \n      \n      max_rfx\n      min_rfx\n      max_rfz\n      min_rfz\n      max_lfz\n      min_lfz\n      range_rfx\n      range_rfz\n      range_lfx\n      range_lfz\n    \n  \n  \n    \n      0\n      179.4015\n      -232.2776\n      1101.3711\n      48.8063\n      2322.2798\n      -13.4557\n      411.6791\n      1052.5648\n      1006.4781\n      2335.7355\n    \n    \n      1\n      140.1327\n      -189.7241\n      1092.3006\n      51.3111\n      2270.0012\n      -13.8138\n      329.8568\n      1040.9895\n      878.2801\n      2283.8150\n    \n    \n      2\n      106.3177\n      -124.4299\n      1117.9434\n      115.4112\n      1942.1915\n      -9.8942\n      230.7476\n      1002.5322\n      1006.2067\n      1952.0857\n    \n    \n      3\n      138.6102\n      -175.8547\n      1102.4140\n      7.9649\n      2509.2788\n      -9.1957\n      314.4649\n      1094.4491\n      1074.0880\n      2518.4745\n    \n    \n      4\n      175.0215\n      -219.5387\n      1119.0327\n      18.2982\n      2492.2496\n      -9.9647\n      394.5602\n      1100.7345\n      1116.0206\n      2502.2143\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      221.6785\n      -114.1301\n      947.5325\n      31.1219\n      1841.7965\n      -7.0486\n      335.8086\n      916.4106\n      762.2835\n      1848.8451\n    \n    \n      633\n      199.9496\n      -113.4915\n      958.0700\n      26.1562\n      1692.9015\n      -8.2001\n      313.4411\n      931.9138\n      730.4450\n      1701.1016\n    \n    \n      634\n      213.2872\n      -112.7565\n      998.6667\n      44.3632\n      1602.5900\n      -5.3440\n      326.0437\n      954.3035\n      726.2761\n      1607.9340\n    \n    \n      635\n      209.7961\n      -122.3425\n      939.1254\n      29.1908\n      1823.2046\n      -6.8408\n      332.1386\n      909.9346\n      801.7169\n      1830.0454\n    \n    \n      636\n      200.5381\n      -165.5618\n      935.7064\n      19.1782\n      1842.2350\n      -8.3528\n      366.0999\n      916.5282\n      786.5035\n      1850.5878\n    \n  \n\n637 rows × 10 columns\n\n\n\nHowever, 10 was arbitrarily chosen, and may not be the best choice. If we run a series of RFE’s which select for 1 more feature than the last we can then plot these values to find our optimal choice for the number of features.\n\n  \nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nimport matplotlib.pyplot as plt\n\n# define data values\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Linear Regression\")\nplt.show() \n\n\n\n\nFrom this chart we can see that we will be able to use a model with 15 features without losing too much accuracy, so we will next use RFE to find out the variables we want to use.\n\nfeat_select = RFE(model, n_features_to_select = 15, step = 1)\nfeat_select.fit(X,y)\nfeat_select.support_\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n       False,  True,  True,  True,  True])\n\n\nThese refer to the following data set:\n\ndata.iloc[:,[42,43,46,47,48,49,50,51,52,53,54,56,57,58,59]]\n\n\n\n\n\n  \n    \n      \n      max_rfx\n      min_rfx\n      max_rfz\n      min_rfz\n      max_lfx\n      min_lfx\n      max_lfy\n      min_lfy\n      max_lfz\n      min_lfz\n      range_rfx\n      range_rfz\n      range_lfx\n      range_lfy\n      range_lfz\n    \n  \n  \n    \n      0\n      179.4015\n      -232.2776\n      1101.3711\n      48.8063\n      121.2052\n      -885.2729\n      130.9304\n      -414.4391\n      2322.2798\n      -13.4557\n      411.6791\n      1052.5648\n      1006.4781\n      545.3695\n      2335.7355\n    \n    \n      1\n      140.1327\n      -189.7241\n      1092.3006\n      51.3111\n      111.2187\n      -767.0614\n      128.0167\n      -475.8343\n      2270.0012\n      -13.8138\n      329.8568\n      1040.9895\n      878.2801\n      603.8510\n      2283.8150\n    \n    \n      2\n      106.3177\n      -124.4299\n      1117.9434\n      115.4112\n      178.4852\n      -827.7215\n      161.8112\n      -437.5895\n      1942.1915\n      -9.8942\n      230.7476\n      1002.5322\n      1006.2067\n      599.4007\n      1952.0857\n    \n    \n      3\n      138.6102\n      -175.8547\n      1102.4140\n      7.9649\n      170.5486\n      -903.5394\n      187.1682\n      -430.2591\n      2509.2788\n      -9.1957\n      314.4649\n      1094.4491\n      1074.0880\n      617.4273\n      2518.4745\n    \n    \n      4\n      175.0215\n      -219.5387\n      1119.0327\n      18.2982\n      176.3782\n      -939.6424\n      177.3536\n      -420.4205\n      2492.2496\n      -9.9647\n      394.5602\n      1100.7345\n      1116.0206\n      597.7741\n      2502.2143\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      221.6785\n      -114.1301\n      947.5325\n      31.1219\n      69.2794\n      -693.0041\n      121.9773\n      -342.0296\n      1841.7965\n      -7.0486\n      335.8086\n      916.4106\n      762.2835\n      464.0069\n      1848.8451\n    \n    \n      633\n      199.9496\n      -113.4915\n      958.0700\n      26.1562\n      60.6210\n      -669.8240\n      129.1773\n      -342.2472\n      1692.9015\n      -8.2001\n      313.4411\n      931.9138\n      730.4450\n      471.4245\n      1701.1016\n    \n    \n      634\n      213.2872\n      -112.7565\n      998.6667\n      44.3632\n      56.2369\n      -670.0392\n      111.4454\n      -329.7390\n      1602.5900\n      -5.3440\n      326.0437\n      954.3035\n      726.2761\n      441.1844\n      1607.9340\n    \n    \n      635\n      209.7961\n      -122.3425\n      939.1254\n      29.1908\n      67.6610\n      -734.0559\n      149.3230\n      -383.7818\n      1823.2046\n      -6.8408\n      332.1386\n      909.9346\n      801.7169\n      533.1048\n      1830.0454\n    \n    \n      636\n      200.5381\n      -165.5618\n      935.7064\n      19.1782\n      91.0192\n      -695.4843\n      150.5726\n      -252.4603\n      1842.2350\n      -8.3528\n      366.0999\n      916.5282\n      786.5035\n      403.0329\n      1850.5878\n    \n  \n\n637 rows × 15 columns\n\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\nfeat_select = RFE(model, n_features_to_select = 15, step = 1)\n\nHowever, linear regression is not the only model that can deal with continous data. We can also use random forest regression. If we use random forest regression, and select 15 features again, we get a higher score than we did with linear regression. We also get slightly different features selected.\n\nfeat_select.fit(x_train, y_train)\nprint(feat_select.score(x_test, y_test))\nfeat_select.support_\n\nHowever, we don’t know if 15 is the optimal amount of features. As such, we can follow the same route we did with linear regression and graph our scores.\n\nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Random Forest Regression\")\nplt.show() \n\nThis shows us that any number of features over 10 will give us a model as good as using more. Therefore, to keep things similar between our two models we can use 15 features."
  },
  {
    "objectID": "posts/Final Project/RandomForestRegressorAnalysis.html",
    "href": "posts/Final Project/RandomForestRegressorAnalysis.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "X = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n#Set up\n\n\nfrom RandomForestRegressor import RandomForest\n\n\nrf = RandomForest()\n\n\nrf.fit(x_train, y_train, 1000, 500)\n\nWe also chose to implement and train a second model on our data. We implemented Random Forest Regression, since it is a very powerful model for making predictions with continous data. When we train this model on our whole data set, we get a much better validation score than with linear regression. However, just like with linear regression we can use feature reduction to reduce overfitting.\n\nrf.score(x_test, y_test)\n\n0.5699951488283441\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\nmodel = RandomForestRegressor()\nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nimport matplotlib.pyplot as plt\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Random Forest Regression\")\nplt.show() \n\n\n\n\nWe can see that this machine doesn’t suffer as much from overfitting, and has a higher validation score than Linear Regression.\n\nx_train = x_train.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\nx_test = x_test.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\n\nrf.fit(x_train, y_train, 1000,500)\nrf.score(x_test, y_test)\n\nUsing the features we found from our RFE we get this score."
  },
  {
    "objectID": "posts/Final Project/index.html",
    "href": "posts/Final Project/index.html",
    "title": "Predicting Bat Speed",
    "section": "",
    "text": "Abstract and Overview of Significance of Topic\nBat speed is a measure of player performance that has become increasingly popular for player development over the past decade. It is typically measured as the speed that the sweet spot of the bat (about 6 inches from the end of the barrel) is traveling when contact is made with the baseball. Bat speed has become increasingly popular due to its high correlation with exit velocity and subsequently hitting metrics such as expected batting average (xBA) or expected weighted on base average (xWOBA). Metrics such as xBA and xWOBA are modern metrics that are used to effectively quantify a players ability to contribute offensively in a positive manner. This increasing popularity in bat speed has led to a related increase in training methodologies based around developing it. Coaches across all levels of play use bat speed as a KPI to validate and inform individualized coaching decisions.\n\n\nFormal Introduction to Topic\nFor our final project, we are using data from the Open Biomechanics Project (OBP) driveline baseball research. Our data captures the biomechanical breakdown of a baseball player’s swing by measuring forces produced by different body parts in three dimensions over exact instances in time, for example, at instance X, player 103’s 2nd swing has a max lead hip force in the y direction of 111.73. The data was captured using a combination of ground force plates, a marker motion capture system, bat speed sensors, and ball flight monitors. The available data is rather robust, accounting for every piece of information that could be responsible for a baseball swing.\nFor our project, our goal is to create a machine learning model that uses this OBP data to identify the most important features of a player’s swing when generating bat speed, and then use those features to accurately predict a player’s bat speed. By comparing an athlete’s true bat speed to their predicted bat speed based on our model, the player could identify how efficiently they are swinging. We hope that this model could be used by baseball players and coaches to address the unique aspects of a player’s swing that could contribute to a higher bat speed, which in turn, would the players reach their fullest potential based on where their inefficiencies lie. Our project can be broken down into two main processes: Identifying the key features that contribute to bat speed. Creating a model that uses the key features to predict bat speed. For the first step, we have decided to run a Recursive Feature Elimination(RFE) on the 60 potential features from our dataset to pull out a smaller number of strong predictive features to use in our model. Next, using those select features, we will run a regression analysis to create a model that can be used to predict a player’s bat speed. Let’s take a closer look at these analyses.\n\n\nValues Statement\nThis project will mainly be utilized by coaches or anybody concerned with player development in baseball. The information that our model would provide to coaches would allow them to make better coaching decisions. In addition to helping coaching staffs, the players themselves would also benefit from more directed training and better evaluation standards. The baseball offseason is very short, so being able to make the most of this time is extremely valuable.\nThis dataset is the first of is the first of its kind to be released as open source. When open-source pitch physics data first became available, it fundamentally changed the way in which baseball was viewed and played. This information allowed both pitchers and hitters to have reliable and measurable feedback on every pitch. The availability of pitch-level biomechanics data has the potential to once again fundamentally change baseball. By working on this project, we are contributing to the larger effort of the baseball community to better understand exactly what makes a baseball swing productive.\n\n\nMaterials and Methods:\n\nData\nFrom the OBP dataset, we will be focusing on baseball-hitting data, specifically force plate and joint velocity to predict bat speed. The original datasets can be found here. Driveline baseball research collected this data using a combination of ground force plates, a marker motion capture system, bat speed sensors, and ball flight monitors. Originally, both the force plate and joint velocity datasets had over 1,000,000 observations, with each individual swing including thousands of observations because the swing was broken down by milliseconds. We felt it was unnecessary to keep the time aspect of the dataset, as the velocities produced for each feature variable were very similar from millisecond to millisecond, and the large datasets were difficult to work with. To get rid of the time component and obtain a more reasonably sized data set, we found the minimum, maximum and range of each feature variable in the dataset for every swing. Each swing is labeled by session_swing in our dataset, and each row is a different swing. The session swing is labeled by player ID and swing number, for example, session_swing 111_3 is player 111’s third swing. Not all players have the same number of swings in the dataset, but we don’t think this should have any impact on our results. After eliminating the time aspect, each swing has 60 potential feature variables. The 60 feature variables include the min, max and range of the forces produced by many different body parts in the x, y and z directions during a player’s swing. Some examples include lead shoulder which is the player’s shoulder closest to the pitcher, and rear hip which is the player’s hip furthest from the pitcher.\nOur data possesses some limitations as it exclusively represents male baseball players and doesn’t include any data from female softball players. We think it would be interesting for Driveline baseball research to expand to softball to eliminate some of the gender bias they have inadvertently caused.\n\n\nRecursive Feature Elimination\nRecursive Feature Elimination, or RFE is a recursive algorithm that is used to find the most impactful features out of a large feature set. This is accomplished by training machine learning on all the features, and then removing the least impactful features. This process is repeated with the smaller feature set, until the feature set is of the desired size. This can help prevent overfitting and allow for easier use and training. This does require that the model it is being used to select features has a way to calculate the effect of features, which means that it won’t work for every model, or some determinator has to be created for it to be used. Another drawback is that unless proper testing is done to find out the amount of impactful features, the accuracy can be diminished beyond the benefits of avoiding overfitting.\n\n\nMultiple Linear Regression\nBecause our project is concerned with predicting bat speed, we require a numeric prediction model, rather than a classification prediction model. We decided to use Multiple Linear Regression, which allows us to take two or more features in a dataset to predict a single numeric dependent variable, bat speed. Multiple Linear Regression differs from regular Linear Regression in that you can use more than one feature to predict the target variable. Once built, we can isolate each individual feature and evaluate its impact on the target variable.\nWith our linear regression model, we will be using the Mean Squared Error (MSE) loss function to determine the accuracy and performance of our model.\n\n\nRandom Forest Regression\nRandom Forest Regression is a technique that creates multiple decision trees and averages their outputs to give a final result that often has a high prediction/classification rate. The process involves the user selecting the amount, n, of decision trees to be created, then using the bootstrapping method to randomly select k data samples from the training set. (Bootstrapping is simply the process of randomly selecting subsets from a dataset over a certain number of iterations and a certain number of variables, then the results from each subset are averaged together which returns a more powerful result.) Then, create n decision trees using different random k values. Using the decision trees, predict regression results that can be used on unseen data. Finally, the regression results are all averaged together, returning a final regression output. Random Forests generally provide high accuracy for feature selection as it generates uncorrelated decision trees built by choosing a random set of features for each tree.\n\n\nVariable Overview\nThe features which we have created our data set with fall into two main categories: biomechanics data and force plate data. Beginning with the biomechanics data, we have a set of joints and their associated angular velocities in three planes of motion. We have information on lead hip, lead shoulder, rear hip, rear shoulder, pelvis, torso, and the torso-pelvis complex. For each of these joints, we calculated the range of velocities and maximum velocities for each swing captured. With the force plate data, the lead foot and rear foot are independently observed, and the data is split among the three planes of motion along the x, y, and z axes. For each pairing of foot and plane of motion, we calculated the minimum, maximum, and range of the force produced.\n\n\n\nThe Process\n\nStep 1: Cleaning the Data\nOur original dataset contained over 1,000,000 observations that were grouped by session_swing. Each swing contained hundreds of observations that analyzed a variety of features over time (~0.0028 seconds between captures). For our project, we wanted to remove this time aspect and instead create a simplified dataset that contained the minimum, maximum, and range values of the features of interest for each swing.\nTo do so, we imported our dataset in R and grouped it by the session_swing variable, and, by using base R functions, calculated the minimum, maximum, and range of each feature variable of interest. We repeated this for the force plate dataset and joint velocity dataset, then used left-join to combine the two datasets to create a conglomerate dataset with all potential predictive features for each session_swing variable.\nWe then added our target vector, max bat speed, from POI_metrics.csv to create our fill dataset that includes our target vector.\nThis process allowed us to get reduce the size of our dataset from over 1,000,000 observations to 665 session_swings.\n\n\nStep 2: RFE Feature Selection\nWe used SKLearn’s RFE feature collection class, which can be found here.\nThe RFE model from the SKLearn class allowed us to gain the base understanding we needed to implement our own version of RFE. After reading through the API and playing around with the RFE feature from SKLearn, we decided to implement our own version of RFE to use with the linear regression model we also implemented. Our RFE function tkaes in three parameters: a feature matrix, X, a target vetor, y, and the number of features we want to be selected, k. The function uses a nested for loop to run through all values i in 1:k and at each iteration, j, checks the weight of all the remaining features that were fit on the linear regression model. From here, the best features are selected as the features with the minimum absolutle value of weight. We use this function in conjunction with our linear regression model to find the number of feautres within our dataset that most accuratley predict the batspeed of a player.\n\n\nStep 3: Building the Models — Multiple Linear Regression and Random Forest\nWe decided to implement our own linear regression model similar to the work we did in class, selecting just the analytic version. We were able to pull out score and predict functions from our previous blog post implementation. We had to modify our fit function by adding a regualarization term to the weight vector to help avoid over/under-fitting the model.\n\n\nStep 4: Testing the Models: Linear Regression and Random Forest\nTo test and train the models, we used an 80/20 train/test split. For both models, we ran a loop to show our the training and testing scores while increasing the number of selected features for the recursive feature elimination model. Once we identified the optimal range of features to use on the Multiple Linear Regression and Random Forest Regression models, we created subsets of our training and testing data to contain the selected features. Finally, we trained and tested our models on the data subsets (80 train/20 split).\n\n\n\nResults and Conclusion\n\nLinear Regression\nBelow shows the effect of increasing the number of features on the accuracy scores produced by the Linear Regression model during RFE. As we can see, our training and testing accuracy scores tend to increase as the number of features increase.\nWe noticed that our model tends to have a higher testing accuracy when the model is ran on fewer features, and has the best training score when the model uses all 60 features. However, because we wanted to identify 10-20 key features, we decided to train and test our model on the 13 best features, which we selected as value that yields the second best testing score.\nThe table shows the 13 most important features as selected by our RFE function. These results were quite unexpected, including values like min_rfx and min_rfy as selected variables of importance. min_rfx and min_rfy represent the minimum rear force produced in the x and y directions, which essentially represent the load, or backwards movement prior to the actual swing itself. Other variables make sense as being some of the most important features, such as range_lead_hip_z, and max_torso_pelvis_x as these are body parts that are essitial in creating the rotational force of a swing to help produce a better bat speed.\nUsing a subset of these 13 features, we trained and tested our model, which produced a testing accuracy score of 38.7%. Unfortunately, this low accuracy indicates that our model isn’t performing as we had hoped. It could mean that our data doesn’t have a strong linear seperability which indicates there is nothing wrong with our model, but rather it isn’t the best model option for our data. Becasue of this, we decided to see if we could produce stronger results by using the Random Forest Regression model.\n\nfrom LinearRegressionAnalytic import LinearRegressionAnalytic\nfrom LinearRegressionAnalytic import rfe\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"biomechanics_dataset_v1.csv\") \n\nnp.random.seed(1)\nX = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n\n\nval_scores = []\ntrain_scores = []\nfor k in range(60):\n    selected_features = rfe(x_train, y_train, k)\n    feature_mask = np.zeros(x_train.shape[1], dtype=bool)\n    #masking to include only the selected features\n    feature_mask[selected_features] = True\n    #subseting x train and test to include only selected feautres\n    X_train_selected = x_train.loc[:, feature_mask]\n    X_test_selected = x_test.loc[:, feature_mask]\n    lr = LinearRegressionAnalytic()\n    #fitting model on selected features\n    lr.fit(X_train_selected, y_train)\n    #appending score to score list\n    val_scores.append(lr.score(X_test_selected, y_test))\n    train_scores.append(lr.score(X_train_selected, y_train))\n\n# plot the results\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 61), val_scores, label='Testing accuracy')\nplt.plot(range(1, 61), train_scores, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Accuracy Scores Produced by Linear Regression Model During RFE\")\nplt.show()\n\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\ndisplay(data.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]])\n\n\nx_train_lin = x_train.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\nx_test_lin = x_test.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\nlr.fit(x_train_lin, y_train)\nprint(\"Testing Accuracy of Subset:\", lr.score(x_test_lin, y_test))\n\n\n\n\n\n\n\n\n  \n    \n      \n      max_rear_shoulder_x\n      max_rear_shoulder_y\n      range_lead_hip_z\n      max_rear_shoulder_z\n      max_torso_y\n      range_lead_shoulder_z\n      max_torso_pelvis_x\n      min_rfx\n      min_rfy\n      range_rear_shoulder_y\n      range_torso_pelvis_x\n      max_lead_hip_z\n      max_pelvis_y\n    \n  \n  \n    \n      0\n      550.0243\n      514.1198\n      778.1339\n      1188.6807\n      182.7302\n      867.6362\n      182.4743\n      -232.2776\n      -88.0097\n      689.2249\n      233.0842\n      384.3450\n      88.3858\n    \n    \n      1\n      638.6019\n      535.2822\n      960.4793\n      1278.5380\n      196.9712\n      1054.5098\n      236.0902\n      -189.7241\n      -106.2254\n      812.9988\n      306.7874\n      520.8627\n      106.4238\n    \n    \n      2\n      580.0406\n      472.9189\n      784.0413\n      1588.7207\n      248.4432\n      988.9415\n      222.8233\n      -124.4299\n      -84.5785\n      708.1030\n      313.2967\n      433.6955\n      82.5397\n    \n    \n      3\n      635.8561\n      484.2663\n      1036.2757\n      888.1270\n      166.9048\n      1472.7250\n      168.7606\n      -175.8547\n      -122.1629\n      732.5588\n      228.6738\n      489.4716\n      81.4764\n    \n    \n      4\n      566.9714\n      502.2202\n      1093.3019\n      1487.6143\n      191.2448\n      1130.6572\n      220.7400\n      -219.5387\n      -72.5831\n      699.1772\n      286.4758\n      597.7220\n      75.9968\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      631.5529\n      488.3580\n      980.3030\n      1575.2948\n      165.8830\n      1150.6032\n      147.9856\n      -114.1301\n      -173.2356\n      804.6660\n      239.9022\n      354.5130\n      124.7927\n    \n    \n      633\n      571.2316\n      477.7701\n      748.3298\n      1604.9299\n      145.5400\n      1026.0944\n      188.9410\n      -113.4915\n      -157.5923\n      735.1128\n      276.8293\n      324.9995\n      137.1521\n    \n    \n      634\n      549.3600\n      407.3251\n      526.3367\n      1393.4961\n      128.0184\n      1029.3547\n      257.2261\n      -112.7565\n      -111.9854\n      584.3304\n      348.2130\n      207.2101\n      128.8111\n    \n    \n      635\n      623.2650\n      463.8467\n      1248.0062\n      1715.0544\n      136.8013\n      892.8699\n      177.4202\n      -122.3425\n      -161.2802\n      725.1355\n      266.6244\n      282.0038\n      157.1024\n    \n    \n      636\n      599.2501\n      505.9937\n      1433.3273\n      1480.4099\n      143.7898\n      1233.8176\n      169.0549\n      -165.5618\n      -132.0637\n      700.1916\n      234.5590\n      500.9032\n      107.8579\n    \n  \n\n637 rows × 13 columns\n\n\n\nTesting Accuracy of Subset: 0.386607442390802\n\n\n\n\nRandom Forest Regression\nBelow is a graph that displays the training and testing scores produced by the Random Rorest Regression model when ran with RFE over all of the features. As we can see from the graph, this model performed much better than the Linear Regression model. Unlike the Linear Regression model, the Random Rorest model reached a peak around 10 features and maintained consistency at that score, whereas the Linear Regression model had increased variance between scores across all of the features.\nBecause the Random Forest Regression model creates a series of trees using the bootstrapping method, we expected the model to have a better training accuracy and weren’t as concerned with overfitting. We also expected it to have a better accuracy score than the Linear Regression model because it can capture non-linear relationships, which we expect our data to have due to the poor performance of the Multiple Linear Regression model. Aditionally, because of the way the trees are built, the random forest model is less likely to be heavily affected by outliers in the data which will allow it to have a better testing accuracy score.\nWe found that the Random Rorest Regression model had the higher training and validation scores than the Linear Regresion model. The training score reached nearly 100% when ran with more than 10 features. The testing accuracy reached around 65% at 10 features and had a slight increase as the number of features increased. To compare this model with the Linear Regression model, we chose the 15 most important features to test and train the Random Forest model. After training and testing our model on the subset of 15 features, we got a testing accuracy score of 55.4%, which is significantly better than our multiple linear regression model. Considering there is no option for our model to randomly guess since we are predicting a continuous numeric value, we are satisfied with the amount our random forest model learned.\nInterestingly, there is only one feature that the Linear Regression model and the Random Forest Regression model both selected: max_rear_shoulder_y. This feature captures the top hand on the bat, so if the shoulder isn’t moving in the swing, it will hinder the ability to produce enough rotational force from the torso.\nWe were suprised to find that, of the 13 features selected by the Linear Regression model and of the 15 features selected by the Random Forest Regression model, there weren’t more similarities amongst the selected features between the two models. We were hoping to discover a few select features that both models identified as important features, and were interested to find that the majority of the selected features were different.\nWe hypothesize that the dimentionality of the forces produced by each body part may be a factor that contributes toward the challenge of distinguishing significant features. For example, in the Linear Regression model, max_toros_y was selected as an important feautre, and in the Random Forest Regression model, max_torso_z was selected as an important feature. This indicates that the max force produced by the torso is important when creating and predicting bat speed. If we were to run this experiment again, we may try getting rid of the x, y and z components and just use the average of the forces produced by that body part.\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\nmodel = RandomForestRegressor()\nscores_test = []\nscores_train = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores_test.append(estm.score(x_test,y_test))\n    scores_train.append(estm.score(x_train,y_train))\n    \nimport matplotlib.pyplot as plt\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores_test, label='Testing accuracy')  # Plot the chart\nplt.plot(x_val, scores_train, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy Score\")\nplt.title(\"Random Forest Regression\")\nplt.legend()\nplt.show() \n\n\nfrom RandomForestRegressor import RandomForest\nrf = RandomForest()\n\nx_train_rf = x_train.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\nx_test_rf = x_test.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\n\nrf.fit(x_train_rf, y_train, 1000,500)\nprint(rf.score(x_test_rf, y_test))\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\ndisplay(data.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]])\n\n0.5541368058167476\n\n\n\n\n\n\n  \n    \n      \n      range_lead_hip_x\n      range_pelvis_y\n      range_rear_shoulder_x\n      range_torso_z\n      range_torso_pelvis_z\n      max_lead_shoulder_z\n      max_pelvis_z\n      max_rear_shoulder_y\n      max_torso_z\n      max_rfz\n      max_lfx\n      range_rfy\n      range_rfz\n      range_lfy\n    \n  \n  \n    \n      0\n      590.6812\n      371.0611\n      838.0101\n      848.3957\n      743.5585\n      617.1386\n      733.6451\n      514.1198\n      775.7749\n      1101.3711\n      121.2052\n      240.6389\n      1052.5648\n      545.3695\n    \n    \n      1\n      536.1970\n      393.4254\n      947.9660\n      814.2556\n      642.8480\n      751.1699\n      799.8748\n      535.2822\n      775.3766\n      1092.3006\n      111.2187\n      297.5680\n      1040.9895\n      603.8510\n    \n    \n      2\n      586.8320\n      396.8130\n      801.1592\n      823.2495\n      853.6754\n      723.6880\n      740.7065\n      472.9189\n      793.0441\n      1117.9434\n      178.4852\n      351.2961\n      1002.5322\n      599.4007\n    \n    \n      3\n      628.4384\n      402.3244\n      958.8471\n      870.6640\n      541.5395\n      810.9479\n      741.3719\n      484.2663\n      819.9890\n      1102.4140\n      170.5486\n      344.0314\n      1094.4491\n      617.4273\n    \n    \n      4\n      595.3172\n      348.1626\n      840.4242\n      809.9368\n      756.6446\n      862.6313\n      770.4950\n      502.2202\n      774.5865\n      1119.0327\n      176.3782\n      262.0008\n      1100.7345\n      597.7741\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      825.4631\n      350.8260\n      1015.7101\n      919.6701\n      587.4149\n      838.6431\n      679.5463\n      488.3580\n      859.0192\n      947.5325\n      69.2794\n      380.5133\n      916.4106\n      464.0069\n    \n    \n      633\n      768.7166\n      377.7951\n      1020.7830\n      910.0289\n      716.7429\n      799.7780\n      715.3288\n      477.7701\n      846.6447\n      958.0700\n      60.6210\n      365.7219\n      931.9138\n      471.4245\n    \n    \n      634\n      667.8735\n      366.3885\n      978.3792\n      880.3159\n      516.4786\n      815.2906\n      701.0455\n      407.3251\n      820.1794\n      998.6667\n      56.2369\n      331.5945\n      954.3035\n      441.1844\n    \n    \n      635\n      698.0434\n      390.7154\n      1029.8385\n      896.8795\n      616.6955\n      604.2217\n      681.9455\n      463.8467\n      838.6638\n      939.1254\n      67.6610\n      383.2294\n      909.9346\n      533.1048\n    \n    \n      636\n      764.1730\n      348.0633\n      1024.9879\n      907.0956\n      606.3088\n      830.5895\n      677.4534\n      505.9937\n      846.7994\n      935.7064\n      91.0192\n      346.1590\n      916.5282\n      403.0329\n    \n  \n\n637 rows × 14 columns\n\n\n\n\n\nConcluding Discussion\nOverall, our project was sucessful, as we built two models and an RFE function to help us determine the most important features of a swing while predicting bat speed. When we formulated the idea for our project idea, our goal was to deliver python source code that we constructed, along with two jupyter notebooks. One that contained our intial exploration, and one that held our final write up and experiments. We were sucessful in meeting this goal, as we finished with more than two jupyter notebooks, and two python source code files that contained our linear regression with RFE and our random forest model.\nIf we had more time, we would work on finding ways to improve the accuracy of our models. One idea we have to improve accuracy score is to get rid of the dimension factor, as we mentioned above, so each biomechanic force only has one representation instead of three. We hope this would help our models narrow down the important features and produce a better accuracy score. Additionally, we would like to bring in more data to train and test our model on. DriveLine baseball technology is relatively new, so the data is sparse. If the technology was more accesible, we could have more data which would allow our model to improve its success.\n\n\n\nBias\nWe created a model that predicts a player’s bat speed with approximately 65% accuracy, using around 10 features. As with any algorithm, we must consider any assumptions or biased data that could have resulted in systematic error throughout our process. Because our training data came from the Driveline Research and Development, our model was only trained on the players with close proximity or eligibility to their biomechanics facility. According to the U.S. Census, 43.3% of the population of Kent, Washington (home of Driveline R&D) are white individuals, contrasted with 12.2% of the population being black individuals. While the data’s goal is to highlight specific bodily forces and movements that contribute to predicting bat speed rather than demographics like race, age, height, or weight, we must acknowledge that this data is most likely skewed toward white individuals and could be less accurate in predicting the bat speed of players of different races.\nAdditionally, we must highlight that baseball is a male-dominated sport, with the rare exception of a few women playing the sport — see article on Alexis Hopkins. While sports are typically gender segregated for the sake of “fairness” and an acknowledgment that male and female bodies are inherently different and will perform as such, factors like absolute strength and size are not as important in the sport of baseball, as they might be in gender-segregated sports like football and soccer. Rather, the Women’s Sports Foundation explains that baseball involves skills that are combinations of timing, coordination, strength, knowledge of the game, strategies, and control, and argues that bat speed and bat control are more important than absolute strength.\nYet, despite all of this, the Driveline R&D data only contains the biomechanics of male batters. Therefore, if our model were to be improved and implemented, it would only perpetuate the segregation of men and women in this sport. If the data to improve a player’s bat speed can only improve male players, women will continue to be left in the dust.\nDriveline Baseball describes its mission as developing the future of baseball training by finding industry-leading insights; all while publishing those insights back to the baseball community where they belong. However, because baseball is a historically white and male-dominated sport, the “insights” that will be found will only contribute to solidifying that the “baseball community” remains dominated by players that fit those demographics.\nIt is our duty to expand this research and development into more marginalized player communities, such as female athletes and athletes of other races. Then, we can use these insights to create unique training programs that empower and embrace their unique features and help them become the best athletes they can be.\n\n\nApplication\nOur bat speed model could be used by coaching staffs to better inform the decisions that they make. For example, if a given player’s predicted bat speed is higher than their actual recorded bat speed, this would indicate a mechanical inefficiency in their swing. Coaches, with this knowledge, could then direct their focus to finding these mechanical ineffeciencies and correcting them. Players in this group would spend more time on skill acquisition training. On the other hand, if a player has a predicted bat speed which is equal to or lower than their actual bat speed, this would indicate above average efficiency in their swing and their training could be directed more towards general strength and power to increase force production. In either case, our model would help streamline the process from data collection to impact, giving coaches and players the power to have impactful training sessions tailored to each individual.\n\n\nGroup Contribution Statement\nCece Ziegler: Helped with data cleaning. Built RFE function and Linear Regression model. Performed RFE and model experiments. Led writing “Results and Conclusion” sections.\nDavid Byrne: Introduced topic. Managed data cleaning. Led writing of “Abstract and Overview of Significance of Topic”, “Data”, “Variable Overview”, “Values Statement”, and “Application” sections.\nJulia Fairbank: Led writing of “Formal Introduction to Topic”, “Recursive Feature Elimination”, “Multiple Linear Regression”, “Random Forest Regression”, “The Process”, and “Bias” sections.\nSam Ehrsam: Conducted initial RFE experiments with SKLearn library. Built Random Forest Regression model. Performed RFE experiments.\n\n\nPersonal Reflection\nThis was an awesome final group project to wrap up my college experience!! I am so grateful and glad to have been apart of this group, everyone was awesome!! I feel like we all contributed in such strong ways and leaned on each other’s strengths in a very productive way. Everyone was very accomadating, communicative, and helpful! We utilized When2Meet.com to handle scheduling across 4 very busy and different calendars and opted into smaller group meetings whenever there was overlap between group members.\nWe also prioritized documenting our process as we went in order to lighten the load on writing the blog post. This decision ended up being a lifesaver because we had detailed explanations for each step, so if a group member couldn’t make a meeting, they would be able to catch-up by reading our process.\nI feel like I learned a ton about collecting and working with data, especially since we had such a massive initial dataset. Our group had so many conversations about making sure we knew exactly the goal of our project and what our target variables were. This allowed us to compress a lot of excess data and focus us more on a subset of data that was easier to work with.\nWe also learned a lot about adapting our plan, especially when our Linear Regression model that we thought would work well produced low accuracy results. While it was difficult to have to adjust our plan and try out the Random Forest Regression model, this flexibility allowed us to compare two different models and actually made our process and discussion more comprehensive.\nWe definitely achieved our goal of creating a working model that predicts bat speed. While it may not have the accuracy that we were hoping for, we learned a ton about problem-solving and implementation along the way. We also met my intial goal of meeting often and submitting all milestones on time.\nI think I will carry a lot of the communication skills we develpoed on this project. A large piece of our success was communicating what we were each interested in working on and everyone having the flexibility to fill in the gaps or pick up slack as needed. I really feel like we were able to efficiently work through this big project quite well, probably the best I’ve ever had for a group project in college, so I am hopeful that I can carry these skills with me as I enter the workforce soon!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451 Final Project: Create a machine learning model that predicts a players bat speed utilizing Recursive Feature Elimination, Linear Regression and Random Forest Regression models.\n\n\n\n\n\n\nMay 14, 2023\n\n\nCece Ziegler, David Byrne, Julia Fairbank, Sam Ehrsam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post discussing the fairness of a prediction algorithm with respect to demographic characteristic bias.\n\n\n\n\n\n\nMay 8, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post reflecting on the work of Timnit Gebru.\n\n\n\n\n\n\nApr 18, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing least-squares linear regression and experimenting with LASSO regularization for overparameterized problems, CSCI 0451.\n\n\n\n\n\n\nMar 27, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing kernel logistic regression, a method for using linear empirical risk minimization to learn nonlinear decision boundaries, CSCI 0451.\n\n\n\n\n\n\nMar 13, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing simple gradient descent and stochastic gradient descent, comparing their performance for training logistic regression, CSCI 0451.\n\n\n\n\n\n\nMar 6, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets, CSCI 0451.\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog: Julia Fairbank is a senior at Middlebury College pursuing a double major in Computer Science and Philosophy. She created this blog to post about CS 0451: Machine Learning."
  }
]