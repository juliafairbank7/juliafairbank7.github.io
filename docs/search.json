[
  {
    "objectID": "mid-course.html",
    "href": "mid-course.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "Julia Fairbank\n\n\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? I’ve missed 2 classes due to being sick, communicated ahead of time.\nHow often have you taken notes on the core readings ahead of the class period? ~60% of the time, I read/skim mostly and take notes in class.\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? ~85% of the time.\nHow many times have you actually presented the daily warm-up to your team? 3 times (3 called for, 1 stepping in).\nHow many times have you asked your team for help while presenting the daily warm-up? Never.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Most days! One of my teammates is a math major and I’ve noticed that he approaches the problems in way I didn’t consider.\nHow often have you helped a teammate during the daily warm-up presentation? A few times!\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? I try to attend Peer Help every week (twice if I have the time!).\n\nHow often have you asked for or received help from your fellow students? A few times, especially on warm-ups or implementing equations for blog posts.\nHave you been regularly participating in a study group outside class? No. \nHow often have you posted questions or answers in Slack? Not much, I prefer going to Peer Help.\n\n\n\n\n\nHow many blog posts have you submitted? 4 (Linear Regression still being a major WIP).\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: 0\nRevisions useful: 1\nRevisions encouraged: 2\nIncomplete: 1\n\nRoughly how many hours per week have you spent on this course outside of class? ~9.\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI chose to focus on implementation, and have been active in attending Peer Help to try to make my code as clean and effective as possible. It takes longer to figure out how to do the work with matrix math as opposed to loops but it makes my code so much more legible and saves me a lot of time in the future.\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nOne of my goals from the beginning of the semester wewasre to submit an attempt on at least all of the blog posts, and submit at least one revision using Peer Help or Student Hours. So far, I have submitted attempts on 4 blog posts, only one of which is at Revision Useful, two being at Revisions Encouraged, and one that hasn’t been graded (and is also a work in progress). However, I’ve realized that spending more time putting together a comprehensible, complete blog post is more valuable than submitting as many submissions as I can. In that sense, I think I will prioritize bulking up and revising the 4 blog posts I currently have submitted before moving on to the next one.\n\n\n\nOne goal that I am proud to say I have kept up with is my attendance in class and Peer Help. I haven’t attended Student Hours yet because my schedule during the day doens’t work well with the hours, but the late night sessions on Mondays and Thursdays work well and I’ve found a lot of success from attending those sessions. I’ve been able to attend the majority of our classes and have kept up with most of the warmups.\n\n\n\nOne of the warmups that I wasn’t able to do was the Project Proposal. Luckily, in the beginning of the semester, I pulled a few data sets and articles of project ideas that I could potentially be interested in, so I was able to talk about a few of those with my teammates during the warmup and discuss their feasibility. I was really excited to hear about David’s project and spoke to him after about joining a group with him and Cece. I think we are a little behind but meeting later today, Wednesday April 5th, to get things moving.\nBecause Cece is in a different section, I think we will have to work extra hard to make sure we are all on the same page and have consistent meeting times.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nOne piece of context that I’ve shared with you is that I am currently in the weeds with my MBA application. I’ve been spending majority of my time putting together my written responses and preparing for the GMAT. This wasn’t something I was sure I was going to do this semester, so my goals for this class didn’t take into account this additional workload. For full transparency, this application will be my top priority for the next 2-3 weeks (final application due April 27th), meaning that my courses will be less of a priority and I will have less time to dedicate to them.\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nGoal 1: 1. Submit an attempt on at least every blog post. 2. Of the 5 (+1 additional) blog posts that are posted currently, I have submitted attempts on 4 of them. 3. I would like to revise this goal to have at least 3 blog posts with “No Revisions Suggested,” and attempts on the majority of the remaining blog posts. I no longer expect/am aiming for submitting attempts on every blog post, and would rather spend more time making sure that I have strong submissions rather than many submissions.\nGoal 2: 1. Attempt to revise each blog post. 2. Of the 4 blog posts I have submitted, I am currently in revision for all 4. 3. I would like to keep this goal because it supports my bigger goal of having stronger submissions over many submissions. This goal will also help me to revise my posts up to my first goal of having at least 3 blog posts with “No Revisions Suggested.”\nGoal 3: 1. Make strong attempts on all of the warm-ups. 2. I have made attempts on ~85% of the warm-ups, missing a few when I had too much work for other classes and wasn’t able to get to them on time. 3. I would like to continue being prepared for class and be a reliable teammate, but acknowledge that I don’t always have time to attempt every warm-up to my full capacity. Considering this, I’d like to spend at least 20 mins on every warm-up, regardless of if I’m able to complete any of it, just so I understand what is being asked and what questions I have to bring to my group. Then, I will be able to be an active participant in the warm-ip activity, even if I don’t have a final answer.\nGoal 4: 1. Attend all the classes that I am physically able to. 2. I have attended all but two classes, both of which I had to miss due to sickness. I communicated ahead of class and coordinated with my teammates to get the notes that I missed from class. 3. I would like to keep this goal as is!\nGoal 5: 1. Attend Student Hours/Peer Help frequently. 2. I try to attend Peer Help weekly and have benefited greatly from working with Steven. 3. I would like to keep this goal up but with an emphasis on Peer Help since those hours work better with my schedule.\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far. Here are some soundbytes to help guide your thinking:\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A-\n\n\n\n\n\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\nI am excited about how much I’ve learned in this class so far! It’s definitely a challenging course for me, but I feel proud of the work I’ve put in on my own, in class, and in Peer Help hours. I am a little disappointed with where I am at numbers wise (having 4 points, 2 on Perceptron + 1 on Gradient Descent + 1 on Logisitic Regression + 0 on Linear Regression) but have shifted my goals to prioritize revisions over getting attempts on my blog posts. I hope by the end of the year I am able to get up to my revised goals and have stronger blog posts that accurately reflect the time and work I put into them."
  },
  {
    "objectID": "posts/Kernel Logistic Regression/index.html",
    "href": "posts/Kernel Logistic Regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "link to source code\nhttps://github.com/juliafairbank7/juliafairbank7.github.io/blob/main/posts/kernel-logistic-regression-post/KernelLogisticRegression.py\n\n\nOverview of Kernel Logistic Regression\nRegular logistic regression is a binary classification machine learning algorithm. One weakness of regular logistic regression is that the algorithm doesn’t work well with data that is not linearly separable. Kernel logistic regression is a technique that extends regular logistic regression to be compatible data that is not linearly separable.\nSimilar to other binary classification machine learning algorithms like perceptron and regular logistic regression, kernel logistic regression aims to predict the label of a data point based on some predictor variables.\nKernel logistic regression requires us to specify a kernel function and parameters for the kernel function. I’ll be using the radial basis function (RBF) kernel function. RBF takes in one parameter, gamma, which controls how “wiggly” the decision boundary should be. Larger gamma means a more wiggly decision boundary.\nThe algorithm uses a stochastic gradient descent to train the kernel logistic regression model, which is an iterative process. We can set the number of training iterations as well as the learning rate to control how much the parameters change on each update.\nOnce the kernel logistic regression model is fully trained, there will be an alpha value for each data point and one bias value.\n\n\nGenerating Non-Linearly Separable Dataset\nBecause kernel logisitic regression is a technique that extends regular logistic regression to be compatible data that is not linearly separable, let’s generate some non-linearly separable data!\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\n\nX, y = make_moons(50, shuffle = True, noise = 0.2)\nplt.scatter(X[:,0], X[:,1], c = y)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\nFrom this dataset, you can start to imagine what the non-linear classification/division might look like. Let’s see how the kernel logisitic regression algorithm attempts to classify that separation.\nBut first, we’re going to use a pre-implemented version of logistic regression and a visualization tool, so let’s import that.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\nLR = LogisticRegression()\nLR.fit(X, y)\n\nplot_decision_regions(X, y, clf = LR)\ntitle = plt.gca().set(title = f\"Accuracy = {(LR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nThis graphs shows a linear regression classification of non-linearly separable data, which cannot perfectly separate non-linearly separable data. Now let’s try with kernel logisitic regression.\n\n\nApplying Kernel Logistic Regression\n\nfrom KernelLogisticRegression import KernelLogisticRegression\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {(KLR.predict(X) == y).mean()}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs you can see, the kernel logisitic regression creates organic, non-linear separations in an attempt to better classify the data. In comparing the accuracy of kernel logisitic regression to linear logistic regression, our accuracy has improved, making this a better classifier for non-linearly separable data.\n\n\nOverview of the KLR Functions\n\n\nfit()\nKernelLogisticRegression.fit(X, y) is the main function that learns the optimal parameters \\({v}\\). To do this, the function first computes a kernel matrix of X with itself, which is used to minimize the empiricial risk and saved as \\({v}\\).\n\n\npredict()\nKernelLogisticRegression.predict(X) takes the dot product of the kernel matrix of X with the value \\({v}\\) from the fit() function, then turns dot product value into a binary value.\n\n\nscore()\nKernelLogisticRegression.score(X, y) uses the predict() function to make a prediction matrix of \\({X}\\) and compute the accuracy.\nNow, let’s experiment with this algorithm. Because we are using RBF, let’s try changing the value of gamma. As a reminder, a larger gamma means a more wiggly decision boundary.\n\n\nExperimenting with Larger Gamma Values\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs you can see, the classifier draws an orange blob around the orange data points rather than drawing a line. This is sufficient to achieve 100% accuracy on the training data.\nHowever, if we were to generate new data, we will see a lower accuracy as the classifier blog remains the same, depiste new data points being generated in different locations.\n\n\nGenerating a New Dataset\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs you can see, the classifier from the first dataset remains the same, despite the new dataset and feature positions. This results in a lower accuracy score since this classifier isn’t made uniquely to fit this dataset.\nNext, let’s try varying the noise, which is a parameter when we call make_moons. The noise determines how spread out the two crescents of points are. Let’s try changing the amount of noise to see the relationship between noise and gamma.\n\n\nExperimenting with Noise\n\n\nNoise = 0\n\nX_lowNoise, y_lowNoise = make_moons(50, shuffle = True, noise = 0)\nplt.scatter(X_lowNoise[:,0], X_lowNoise[:,1], c = y_lowNoise)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nplt.title(\"Testing Dataset, Noise = 0\")\n\nText(0.5, 1.0, 'Testing Dataset, Noise = 0')\n\n\n\n\n\n\nKLR_lowG = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR_lowG.fit(X_lowNoise, y_lowNoise)\nplot_decision_regions(X_lowNoise, y_lowNoise, clf = KLR_lowG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_lowG.score(X_lowNoise, y_lowNoise)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Noise = 0, Gamma = 0.1', fontsize=14, y=1)\n\nText(0.5, 1, 'Noise = 0, Gamma = 0.1')\n\n\n\n\n\n\nKLR_highG = KernelLogisticRegression(rbf_kernel, gamma = 100)\nKLR_highG.fit(X_lowNoise, y_lowNoise)\nplot_decision_regions(X_lowNoise, y_lowNoise, clf = KLR_highG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_highG.score(X_lowNoise, y_lowNoise)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Noise = 0, Gamma = 100', fontsize=14, y=1)\n\nText(0.5, 1, 'Noise = 0, Gamma = 100')\n\n\n\n\n\nWith no noise, a low gamma value (0.1) has a perfect accuracy and increasing it has no affect.\n\n\nNoise = 1.0\n\nX_highNoise, y_highNoise = make_moons(50, shuffle = True, noise = 1.0)\nplt.scatter(X_highNoise[:,0], X_highNoise[:,1], c = y_highNoise)\nlabels = plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nplt.title(\"Testing Dataset, Noise = 1.0\")\n\nText(0.5, 1.0, 'Testing Dataset, Noise = 1.0')\n\n\n\n\n\n\nKLR_lowG = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR_lowG.fit(X_highNoise, y_highNoise)\nplot_decision_regions(X_highNoise, y_highNoise, clf = KLR_lowG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_lowG.score(X_highNoise, y_highNoise)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Noise = 1.0, Gamma = 0.1', fontsize=14, y=1)\n\nText(0.5, 1, 'Noise = 1.0, Gamma = 0.1')\n\n\n\n\n\n\nKLR_highG = KernelLogisticRegression(rbf_kernel, gamma = 100)\nKLR_highG.fit(X_highNoise, y_highNoise)\nplot_decision_regions(X_highNoise, y_highNoise, clf = KLR_highG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_highG.score(X_highNoise, y_highNoise)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Noise = 1.0, Gamma = 0.1', fontsize=14, y=1)\n\nText(0.5, 1, 'Noise = 1.0, Gamma = 0.1')\n\n\n\n\n\nWhen we increase noise at 0.5, a low gamma value (0.1) yields a lower accuracy that improves as gamma increases to a higher value.\n\n\nConcentric Circles Dataset\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.8, noise=.05)\n\nfig_c = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nKLR_c_lowG = KernelLogisticRegression(rbf_kernel, gamma = 0.1)\nKLR_c_lowG.fit(X_c, y_c)\nplot_decision_regions(X_c, y_c, clf = KLR_c_lowG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_c_lowG.score(X_c, y_c)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Gamma = 0.1', fontsize=14, y=1)\n\nText(0.5, 1, 'Gamma = 0.1')\n\n\n\n\n\n\nKLR_c_highG = KernelLogisticRegression(rbf_kernel, gamma = 100)\nKLR_c_highG.fit(X_c, y_c)\nplot_decision_regions(X_c, y_c, clf = KLR_c_highG)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR_c_highG.score(X_c, y_c)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\nplt.suptitle('Gamma = 100', fontsize=14, y=1)\n\nText(0.5, 1, 'Gamma = 100')\n\n\n\n\n\nFor concentric circle datasets where the features are closely concentric, a higher gamma value doesn’t always improve the accuracy of the classification. KLR with low gamma values still yield high accuracies that don’t necessarily improve by increasing the gamma."
  },
  {
    "objectID": "posts/Logistic Regression/index.html",
    "href": "posts/Logistic Regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "link to source code\nhttps://github.com/juliafairbank7/juliafairbank7.github.io/blob/main/posts/logistic-regression-post/LogisticRegression.py\n\n\nOptimization of Logistic Regression\nLogistic regression is a classifier that estimates the probability of something based on a given dataset of independent variables.\nIn this blog post, I will implement a simple gradient descent and a stochastic gradient descent and will compare their performance for training logistic regression.\n\n\nApplying Logistic Regression\nLet’s start by generating a dataset that we will use on our gradient descent and our stochastic gradient descent.\n\nfrom LogisticRegression import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.seterr(all='ignore') \n\nnp.random.seed(777)\n\nn = 100\np_features = 3\n\nX1, y1 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\n\nGradient Descent\nWhen we call LR.fit(X, y), LR should have an instance variable of weights called w, which is a vector of weights, including the bias term. LR should have an instance variable called LR.loss_history which is a list of the evolution of the loss over the training period, and an instance variable called LR.score_history, which is a list of the evolution of the score over the training period.\nLet’s see how this works on our dataset.\n\nnp.random.seed(777)\n\nn = 100\np_features = 3\n\nX1, y1 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\nLR_f = LogisticRegression()\nLR_f.fit(X1, y1, alpha = 0.1, max_epochs = 1000)\n\nfig = plt.scatter(X1[:,0], X1[:,1], c = y1)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color = \"black\")\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nfig1 = draw_line(LR_f.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Gradient Descent\")\n\nText(0.5, 1.0, 'Gradient Descent')\n\n\n\n\n\nAs you can see, our simple gradient descent has drawn its classifying line to separate the two features. It’s not a perfect classifier, but let’s see what the loss and score values look like.\n\nprint(LR_f.loss_history[-3])\nprint(LR_f.score_history[-1])\n\n0.02319692785879772\n0.99\n\n\nWe have a low loss and high score, meaning this line is a good strong separating line for logisitic regression.\nNow, let’s compare this with the stochastic gradient descent.\n\n\nStochastic Gradient Descent\nLogisticRegression.fit_stochastic(X, y) is an alternative version of the fit() method which computes a stochastic gradient by picking a random subset, computing the stochastic gradient, performing an update, then repeating. When LR.fit_stochastic(X, y) is called, LR should have an instance variable of weights called w, which is a vector of weights, including the bias term b. LR should have an instance variable called LR.loss_history which is a list of the evolution of the loss over the training period, and an instance variable called LR.score_history, which is a list of the evolution of the score over the training period.\n\n\nTesting Stochastic\nAgain, let’s start by re-generating that same random dataset.\n\nnp.random.seed(777)\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\nLR_s = LogisticRegression()\nLR_s.fit_stochastic(X2, y2, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color = \"black\")\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig2 = draw_line(LR_s.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Stochastic Gradient Descent')\n\n\n\n\n\nAs you can see, stochastic gradient descent has drawn its line of best fit to separate the two features. This line seems to separate the data better than the simple gradient, so let’s look at the score and loss values to compare more accurately. Since stochastic gradient descent, we’ll look at three of the loss values to see how it changes over iterations.\n\nprint(LR_s.loss_history[-3])\nprint(LR_s.loss_history[-2])\nprint(LR_s.loss_history[-1])\nprint(LR_s.score_history[-1])\n\n0.014117604281679486\n0.014115400515806372\n0.014113204314421224\n1.0\n\n\nAgain, we have very low loss values that get smaller over iterations, and a perfect score, making the stochastic gradient descent a great classifier that had better results classifying this dataset than the simple gradient descent.\nFor sake of visualization and comparision, let’s plot the two fit methods on the same graph.\n\n\nComparison Plotting\n\nnp.random.seed(777)\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\nLR_f_b = LogisticRegression() #red\nLR_f_b.fit(X3, y3, alpha = 0.1, max_epochs = 1000)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color = \"red\", label = 'gradient')\n\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nfig3 = draw_line(LR_f_b.w_hat, -2, 2)\n\n\nLR_s_b = LogisticRegression() #blue\nLR_s_b.fit_stochastic(X3, y3, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"blue\", label='stochastic')\n\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nfig3 = draw_line(LR_s.w_hat, -2, 2)\nplt.title(\"Comparing Gradient Descent with Stochastic Gradient Descent\")\n\nlegend = plt.legend() \n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X3, y3, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, color = \"blue\", label = \"stochastic gradient; alpha = 0.05\")\n\nLR.fit_stochastic(X3, y3, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, color = \"teal\", label = \"stochastic gradient; alpha = 0.1\")\n\nLR.fit(X3, y3, alpha = .05, max_epochs = 10000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, color = \"red\", label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \nplt.title(\"Comparing Loss History Across Fit Methods\")\n\nText(0.5, 1.0, 'Comparing Loss History Across Fit Methods')\n\n\n\n\n\nFrom this graph, we can see that the stochastic method converges faster than gradient descent with a steeper decline. The gradient method takes longer and requires more epochs to find a good solution.\n\n\nExperimenting\nLet’s try experimenting with increasing our alpha value.\n\nnp.random.seed(777)\n\nX4, y4 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig4 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\n#large alpha value\nLR_f_a_large = LogisticRegression()\nLR_f_a_large.fit(X4, y4, alpha = 100, max_epochs = 10000)\n\n#normal alpha value\nLR_f_a_norm = LogisticRegression()\nLR_f_a_norm.fit(X4, y4, alpha = .1, max_epochs = 10000)\n\nfig4 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"maroon\", label='large alpha, a = 100')\n\n#alpha = 200\nfig4 = draw_line(LR_f_a_large.w_hat, -2, 2)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"rosybrown\", label='normal alpha, a = 0.1')\n    \n#alpha = 0.01\nfig4 = draw_line(LR_f_a_norm.w_hat, -2, 2)\n\nlegend = plt.legend() \nplt.title(\"Experimenting with Large Alpha Value\")\n\n\nText(0.5, 1.0, 'Experimenting with Large Alpha Value')\n\n\n\n\n\n\nxlab = plt.xlabel(\"Number of Iterations\")\nylab = plt.ylabel(\"Loss History Score\")\n\nnum_steps1 = len(LR_f_a_large.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR_f_a_large.loss_history, color = \"maroon\", label = 'large alpha, a = 100')\n\nnum_steps2 = len(LR_f_a_norm.loss_history)\nplt.plot(np.arange(num_steps2) + 1, LR_f_a_norm.loss_history, color = \"rosybrown\", label = 'normal alpha, a = 0.1')\n\nlegend = plt.legend() \n\n\n\n\n\n\nExperimenting with Non-Linearly Separable Data\nWe’ve seen how our simple gradient descent and stochtastic gradient descent work on linearly separable data, but let’s see how they compare on non-linearly separable data.\nFirst, let’s generate some non-linearly separable data.\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.3, noise=.05)\n\nfig_c = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR_f_NLS_a_large = LogisticRegression()\nLR_f_NLS_a_large.fit(X_c, y_c, alpha = 100, max_epochs = 10000)\n\nLR_f_NLS_a_norm = LogisticRegression()\nLR_f_NLS_a_norm.fit(X_c, y_c, alpha = 0.1, max_epochs = 10000)\n\nfig_c = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"maroon\", label='large alpha, a = 100')\n\nfig_c = draw_line(LR_f_NLS_a_large.w_hat, -2, 2)\n\ndef draw_line(w_hat, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w_hat[0]*x + w_hat[2])/w_hat[1]\n    plt.plot(x, y, color=\"rosybrown\", label='normal alpha, a = 0.1')\n\nfig_c = draw_line(LR_f_NLS_a_norm.w_hat, -2, 2)\n\n\n\n\n\nxlab = plt.xlabel(\"Number of Iterations\")\nylab = plt.ylabel(\"Loss History Score\")\n\nnum_steps1 = len(LR_f_NLS_a_large.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR_f_NLS_a_large.loss_history, color = \"maroon\", label = 'large alpha, a = 100')\n\nnum_steps2 = len(LR_f_NLS_a_norm.loss_history)\nplt.plot(np.arange(num_steps2) + 1, LR_f_NLS_a_norm.loss_history, color = \"rosybrown\", label = 'normal alpha, a = 0.1')\n\nlegend = plt.legend() \n\n\n\n\nAs you can see, the large alpha doesn’t appear to work in the way it should (converges before it begins), while the normal alpha seems to separate the features with a low-ish loss score.\n\n\nExperimenting with Batch Size\n\nnp.random.seed(777)\n\nX5, y5 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig5 = plt.scatter(X5[:,0], X5[:,1], c = y5)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Testing Dataset\")\n\nText(0.5, 1.0, 'Testing Dataset')\n\n\n\n\n\n\nLR_smallBatch = LogisticRegression()\nLR_smallBatch.fit_stochastic(X5, y5, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 5, \n                  alpha = .1) \n\nnum_steps = len(LR_smallBatch.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_smallBatch.loss_history, color = \"slategray\", label = \"small batch size, batch_size = 5\")\n\nLR_largeBatch = LogisticRegression()\nLR_largeBatch.fit_stochastic(X5, y5, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 50, \n                  alpha = .1) \n\nnum_steps = len(LR_largeBatch.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_largeBatch.loss_history, color = \"navy\", label = \"large batch size, batch_size = 50\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Number of Iterations\")\nylab = plt.ylabel(\"Loss History Score\")\nplt.title(\"Comparing Batch Size on Stochastic Gradient Descent\")\n\nlegend = plt.legend() \n\n\n\n\nIn this experiment, you can see that the larger batch size converges at a faster rate than the small batch size. However, they both appear converge at the same amount of epochs."
  },
  {
    "objectID": "posts/Linear Regression/index.html",
    "href": "posts/Linear Regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "link to source code\nhttps://github.com/juliafairbank7/juliafairbank7.github.io/tree/main/posts/linear-regression-post\n\n\nOverview of Linear Regression\nWith data that has a linear relationship between two variables, linear regression allows you to find the best line that fits that relationship. The Least Squares Regression Line is the line that minimizes the variance (you can think about it like vertical height) between the data points to the regression line. This best line of fit minimizes the (It’s called a “least squares” because the variance represents the sum of squares of the errors).\nLeast-squares linear regression is a convex linear model that makes predictions of the form \\(\\tilde{y}_i = \\langle w, {x}_{i} \\rangle\\).\nThe loss function that we will be using is \\(l(\\tilde{y}, y) = (\\tilde{y} - y)^{2}\\), which says that the loss is equal to the squared error (least squares!)\nOur empirical loss minimization function is\n$ = $ arg min \\(L(w)\\)\nThere are multiple ways to solve this empirical loss minimization function, and we will be focusing on two of them: analytically and using gradient descent.\nSo, we will be implementing least-squares linear regression in these two different ways: 1. Analytical Fit 2. Gradient Descent Fit\n\n\nAnalytical Fit\nFor the analytical implementation of least-squares linear regression, we can use an explicit formula that uses matrix inversion. This formula, \\(\\hat{w} = {X}^{T}{X}^{-1}X^{T}y\\), will calculate our optimal weight vector.\n\n\nGradient Descent Fit\nFor the gradient descent implementation of least-squares linear regression, we will need to compute the gradient with respect to \\(\\hat{w}\\) and repeat until convergence. To do this without killing our computer by over-computing, we can calculate \\(P = {X}^{T}{X}\\) and \\(q = {X}^{T}y\\) just once. From there, our gradient is \\(2(Pw-q)\\).\nBy calculating our gradient this way, we are eliminating the chance for the run-time to shoot up depending on the number of data points.\n\n\nDemo Linear Regression on Data\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom LinearRegression import LinearRegression\n\nLinReg = LinearRegression()\n\nLet’s take a look at how our analytical fit linear regression works on some sample data.\n\n\nAnalytical Fit Demo\n\nw0 = -0.5\nw1 =  0.7\n\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1) \n\n\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nplt.scatter(x, y)\n\nLinReg.fit_analytic(x, y)\n\nx_fake = np.linspace(0, 1, 101)[:,np.newaxis]\n\npredictions = LinReg.predict(x_fake)\n\nplt.plot(x_fake, predictions, color = \"black\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\nAs you can seen through the graph above, the analytical fit has classified the least squares regression line through our data points. Now let’s see how our other method works.\n\n\nGradient Descent Fit Demo\n\nw0 = -0.5\nw1 =  0.7\n\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1) \n\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nplt.scatter(x, y)\n\nLinReg.fit_gradient(x, y)\n\nx_fake = np.linspace(0, 1, 101)[:,np.newaxis]\n\npredictions = LinReg.predict(x_fake)\nprint(predictions.shape)\n\nplt.plot(x_fake, predictions, color = \"blue\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n(101, 1)\n\n\n\n\n\nHere, our gradient descent method draws its least squares regression line through our data points. Let’s see how our score value has changed over our epochs.\n\n\nScore History of Gradient Descent Fit Score over Iterations\n\nplt.plot(LinReg.score_history, color = \"blue\");\n\n\n\n\n\n\nCreate Testing & Validation Data\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow, let’s use this testing and validation data to see our testing and validation scores from our analytical fit.\n\nfrom LinearRegression import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train)\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = -142.5592\nValidation score = -145.883\n\n\nWe can also see our training and validation scores from our gradient descent fit.\n\nLR2 = LinearRegression()\nLR2.fit_gradient(X_train, y_train, alpha = 0.00003, max_epochs = 2000)\n\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.3969\nValidation score = 0.3531\n\n\nLet’s see how our gradient descent score has changed over our iterations (epochs).\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(title = \"Score History for Gradient Descent Fit\", xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\nExperimenting with the Number of Features Used\nNow that we’ve seen how linear regression works on 1 feature and a bias term, let’s try to increase the number of features to 5.\n\ndef LR_analytic_features(n_train = 100, n_val = 100, p_features = 5, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nn_train = 1000\nn_val = 100\nnoise = 0.2\n\nfeature_count = []\ntesting_scores = []\nvalidation_scores = []\n    \nfor i in range(n_val-1):\n    p_features = i\n    feature_count.append(i)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR_analytic_features = LinearRegression()\n    LR_analytic_features.fit_analytic(X_train, y_train)\n    testing_scores.append(LR_analytic_features.score(X_train, y_train).round(4))\n    validation_scores.append(LR_analytic_features.score(X_val, y_val).round(4))\n\nplt.plot(testing_scores)\nplt.plot(validation_scores)\nlabels = plt.gca().set(title = \"Increasing Features on Analytic Fit\", xlabel = \"Features\", ylabel = \"Score\")\n\n\n\n\n\ndef LR_GD_features(n_train = 100, n_val = 100, p_features = 5, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nn_train = 1000\nn_val = 100\nnoise = 0.2\n\nfeature_count = []\ntesting_scores = []\nvalidation_scores = []\n    \nfor i in range(n_val-1):\n    p_features = i\n    feature_count.append(i)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR_GD_features = LinearRegression()\n    LR_GD_features.fit_gradient(X_train, y_train, alpha = 0.00003, max_epochs = 40000)\n    testing_scores.append(LR_GD_features.score(X_train, y_train).round(4))\n    validation_scores.append(LR_GD_features.score(X_val, y_val).round(4))\n\nKeyboardInterrupt: \n\n\n\nplt.plot(validation_scores)\nplt.plot(testing_scores)\nlabels = plt.gca().set(title = \"Increasing Features on Gradient Descent Fit\", xlabel = \"Features\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/Perceptron/index.html",
    "href": "posts/Perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Overview of Perceptron Model\nThe Peceptron Algorithm is a binary, linear classification machine learning algorithm. The algorithms aims to find a rule that separates two distinct groups in some data.\nThe algorithm takes in a vector of n data points that have k features as input and predicts a label by calculating the weighted sum of the inputs and a bias b, and predicts 1 if positive and 0 if negative.\nThe perceptron algorithm aims to find a good (not always the best) \\(\\tilde{w}\\) using the following algorithm:\n\nBegin with some random \\(\\tilde{w}^{(0)}\\)\n“Until we’re done” in some time-step t:\n\nPick a random data point i within the n data points\nCompute \\(\\hat{y}_{i}^{(t)} = \\langle \\tilde{w}^{(t)}, \\tilde{x}_{i} \\rangle\\)\nIf \\(\\hat{y}_{i}^{(t)}{y}_{i}\\) > 0, then point is correctly classified – pass!\nIf \\(\\hat{y}_{i}^{(t)}{y}_{i}\\) < 0, then perform the update:\n\n\\[\n     \\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + {y}_i \\tilde{w}_{i}\n    \\]\n\nIn order to actually separate the two classes, the algorithm uses the prediction function which follows the rule: $ ^{(1)}, _{(i)} _i = 1 $,\nwhich says that if the dot product between \\(\\tilde{w}^{(1)}\\) and \\(\\tilde{w}_{(i)}\\) is less than 0, label it 1, otherwise label it -1.\nLet’s see how my perceptron algorithm works on a set of data.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.gca().set(title = \"Data Points\")\n\n\n\n\n\n\nApplying the fit() method\nPerceptron.fit(X, y) is my primary method. When p.fit(X, y) is called, p should have an instance variable of weights called w. This w is the vector in the classifier above. Additionally, p should have an instance variable called p.history which is a list of the evolution of the score over the training period.\nWithin the fit() function, we perform the perceptron update that correspondes to Equation 1, which states:\n\\[\n\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb1 (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{w}_{(i)} \\rangle \\lt 0)  \\tilde{y}_i \\tilde{x}_i\n\\]\n\n\napplying the predict() method\nPerceptron.predict(X) returns a vector of predicted labels on the data using the function \\[  \\langle \\tilde{w}^{(1)}, \\tilde{w}_{(i)} \\rangle \\lt 0 \\iff {y}_i = 1\n\\]\nSee the linear classification below that separates the two classes.\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#p.score(X, y)\n\n\n\n\n\n\napplying the score() method\nPerceptron.score(X, y) returns the accuracy of the perceptron as a number between 0 and 1, with 1 corresponding to perfect classification. We can see how the algorithm updates through different accuracies/losses, then ultimately converges to 0 with a perfect classification. The algorithm converges at a loss of 0 when it has successfully located a hyperplane that perfectly separates the two classes.\n\n# import Perceptron from source.py\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nHowever, this algorithm only converges when the data is lineraly separable, meaning there exists some line that separates the two classes. So let’s take a look at a nonlineraly separable dataset to see how the algorithm responds.\n\n\nnon-lineraly separable data\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.3, noise=.05)\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs you can see from this datset, there is no straight line that could separate the two classes, making this a nonlinearly separable dataset. Let’s see how the perceptron algorithm fits this data and where it draws the hyperplane.\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAs you can see, the line drawn by the algorithm does not perfectly separate the two classes. Let’s look at the loss history graph to see the different updates.\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nAs you can see in the graph above, the algorithm never converges to 0, proving that the algorithm does not work on non-linearly separable datasets.\nNext, let’s see how the algorithm handles datasets with higher dimensions.\n\n\n5-Dimension Data\n\nnp.random.seed(12345)\n\nn = 100\np_features = 5\n\nX_5, y_5 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -3, 4, 1), (1.7, 1.7, 1, 53, 5)])\n\np = Perceptron()\np.fit(X_5, y_5, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nAs you can see in this loss history graph, the algorithm converges to 0 around iteration 65. Because the algorithm eventually has a loss of 0, this means there exists a hyperplane that perfectly separates the two classes. So, the perceptron algorithm works on multi-dimensional data, as long as it is lineraly separable.\n\n\nrun-time complexity()\nFrom Equation 1,\n\\[\n\\tilde{w}^{(t+1)} = \\tilde{w}^{(t)} + \\mathbb1 (\\tilde{y}_i \\langle \\tilde{w}^{(t)}, \\tilde{w}_{(i)} \\rangle \\lt 0)  \\tilde{y}_i \\tilde{x}_i\n\\]\nthe run-time complexity of one iteration would be \\(O({p})\\), because the dot product of \\(\\langle \\tilde{w}^{(t)}, \\tilde{w}_{(i)} \\rangle\\) takes \\(O({p})\\) time, the dot product of \\(\\tilde{y}_i \\tilde{x}_i\\) takes \\(O({p})\\) time, and \\(\\tilde{w}^{(t)}\\) takes \\(O({p})\\) time, for a total of \\(O({3p})\\) time, which simplifies to \\(O({p})\\).\nFrom this , we know that the runtime complexity depends only on the number of features, \\({p}\\), and doesn’t depend on the number of datapoints, \\({n}\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post reflecting on the work of Timnit Gebru.\n\n\n\n\n\n\nApr 18, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing least-squares linear regression and experimenting with LASSO regularization for overparameterized problems, CSCI 0451.\n\n\n\n\n\n\nMar 27, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing kernel logistic regression, a method for using linear empirical risk minimization to learn nonlinear decision boundaries, CSCI 0451.\n\n\n\n\n\n\nMar 13, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing simple gradient descent and stochastic gradient descent, comparing their performance for training logistic regression, CSCI 0451.\n\n\n\n\n\n\nMar 6, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets, CSCI 0451.\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog: Julia Fairbank is a senior at Middlebury College pursuing a double major in Computer Science and Philosophy. She created this blog to post about CS 0451: Machine Learning."
  }
]