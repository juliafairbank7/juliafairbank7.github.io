[
  {
    "objectID": "posts/logistic-regression-post/index.html",
    "href": "posts/logistic-regression-post/index.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom LogisticRegression import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# inspect the fitted value of w\n#LR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 50, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/perceptron-blog-post/index.html",
    "href": "posts/perceptron-blog-post/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "fit()\nPerceptron.fit(X, y) is my primary method. When p.fit(X, y) is called, p should have an instance variable of weights called w. This w is the vector in the classifier above. Additionally, p should have an instance variable called p.history which is a list of the evolution of the score over the training period.\n\ndef fit(self, X, y, max_steps=1000):\n    #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n       \n     #make -1 or +1 \n    y_hat = y*2-1\n    \n    #initialize random w vector\n    self.w_hat = np.random.rand(X_hat.shape[1]) \n    self.history = []\n        \n    for _ in range(max_steps):\n    \n        i = np.random.randint(0, X.shape[0])\n            \n        #update\n        self.w_hat = (\n            self.w_hat \n            + ((y_hat[i]*np.dot(self.w_hat, X_hat[i]) <0)*1)\n            * y_hat[i]\n            * X_hat[i]\n        )\n            \n        loss = 1 - self.score(X, y)\n        self.history.append(loss)\n            \n        if loss == 0:\n            break\n\n\n\nperceptron update\nIn the fit() function, I performed the perceptron update that corresponded Equation 1. In the function, I added updated \\(\\hat{w}\\) by adding \\(\\hat{w}\\) to binary value of vector \\(\\hat{y}\\) at instance \\(i\\) multipled by the dot product of \\(\\hat{w}\\) and \\(\\hat{X}\\) at instance \\(i\\), multipled by vector \\(\\hat{y}\\) at instance \\(i\\), multiplied by \\(\\hat{X}\\) at instance \\(i\\).\n\n\npredict()\nPerceptron.predict(X) returns a vector of predicted labels on the data.\n\ndef predict(self, X):\n     #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    prediction_vector = X_hat @ self.w_hat\n        \n    prediction_vector = prediction_vector > 0 #true false\n        \n    prediction_vector = prediction_vector * 1 #1 0\n        \n    return prediction_vector\n\n\n\nscore()\nPerceptron.score(X, y) returns the accuracy of the perceptron as a number between 0 and 1, with 1 corresponding to perfect classification.\n\ndef score(self, X, y):\n    #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    predictions = self.predict(X)\n        \n    accuracy = predictions == y \n        \n    accuracy = accuracy * 1\n        \n    accuracy = accuracy.mean()\n        \n    return accuracy\n\n\n\nEXPERIMENTS\n\n\nlineraly separable data\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.gca().set(title = \"Data Points\")\n\n\n\n\n\n# import Perceptron from source.py\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#p.score(X, y)\n\n\n\n\n\n\nnon-lineraly separable data\nIn this experiment, I am testing the perceptron algorithm on non-lineraly separable data. I chose to create a circular dataset of where one feature is a concentric circle within the other, and could not be separated by a line.\nAs shown by the loss graph below, this experiment proves that the algorithm does not converge on a non-lineraly separable data set.\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.3, noise=.05)\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\n\n\n5-Dimension Data\nIn this experient, I am increasing the dimensions from 2-dimensions to 5-dimensions. As shown below by the loss graph, I am showing that the data remains linterally separable as the function ultimately reaches a 0 zero loss and terminates around iteration 65.\n\nnp.random.seed(12345)\n\nn = 100\np_features = 5\n\nX_5, y_5 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -3, 4, 1), (1.7, 1.7, 1, 53, 5)])\n\n\np = Perceptron()\np.fit(X_5, y_5, max_steps=1000)\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets, CSCI 0451.\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog: Julia Fairbank is a senior at Middlebury College pursuing a double major in Computer Science and Philosophy. She created this blog to post about CS 0451: Machine Learning."
  }
]