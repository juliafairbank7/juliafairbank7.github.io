[
  {
    "objectID": "posts/logistic-regression-post/LogisticRegression.html",
    "href": "posts/logistic-regression-post/LogisticRegression.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "from LogisticRegression import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# inspect the fitted value of w\n#LR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()"
  },
  {
    "objectID": "posts/logistic-regression-post/index.html",
    "href": "posts/logistic-regression-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/logistic-regression-post/index.html#math",
    "href": "posts/logistic-regression-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron-blog-post/index_old.html",
    "href": "posts/perceptron-blog-post/index_old.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# import Perceptron from source.py\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\n\n#p.w\n\n#print(p.history[-10:]) #just the last few values\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#p.score(X, y)\n\n\n\n\n\n\nfrom sklearn.datasets import make_circles\n\nX_c, y_c = make_circles(n_samples=400, factor=.3, noise=.05)\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np = Perceptron()\np.fit(X_c, y_c, max_steps=1000)\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X_c[:,0], X_c[:,1], c = y_c)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n#p.score(X, y)\n\n\n\n\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX_5, y_5 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7, -3, 4, 1), (1.7, 1.7, 1, 53, 5)])\n\n\np = Perceptron()\np.fit(X_5, y_5, max_steps=1000)\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")"
  },
  {
    "objectID": "posts/perceptron-blog-post/index copy.html",
    "href": "posts/perceptron-blog-post/index copy.html",
    "title": "Perceptron",
    "section": "",
    "text": "\"CSCI\" + \" 0451\"\n\n'CSCI 0451'"
  },
  {
    "objectID": "posts/perceptron-blog-post/index copy.html#perceptron-algorithm",
    "href": "posts/perceptron-blog-post/index copy.html#perceptron-algorithm",
    "title": "Perceptron",
    "section": "Perceptron Algorithm",
    "text": "Perceptron Algorithm"
  },
  {
    "objectID": "posts/perceptron-blog-post/index.html",
    "href": "posts/perceptron-blog-post/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.gca().set(title = \"Data Points\")\n\n\n\n\n\nfrom perceptron import Perceptron\n\n\nfit()\nPerceptron.fit(X, y) is my primary method. When p.fit(X, y) is called, p should have an instance variable of weights called w. This w is the vector in the classifier above. Additionally, p should have an instance variable called p.history which is a list of the evolution of the score over the training period.\n\n\ndef fit(self, X, y, max_steps=1000):\n    #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n       \n     #make -1 or +1 \n    y_hat = y*2-1\n    \n    #initialize random w vector\n    self.w_hat = np.random.rand(X_hat.shape[1]) \n    self.history = []\n        \n    for _ in range(max_steps):\n    \n        i = np.random.randint(0, X.shape[0])\n            \n        #update\n        self.w_hat = (\n            self.w_hat \n            + ((y_hat[i]*np.dot(self.w_hat, X_hat[i]) <0)*1)\n            * y_hat[i]\n            * X_hat[i]\n        )\n            \n        loss = 1 - self.score(X, y)\n        self.history.append(loss)\n            \n        if loss == 0:\n            break\n\n\n\npredict()\nPerceptron.predict(X) returns a vector of predicted labels on the data.\n\ndef predict(self, X):\n     #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    prediction_vector = X_hat @ self.w_hat\n        \n    prediction_vector = prediction_vector > 0 #true false\n        \n    prediction_vector = prediction_vector * 1 #1 0\n        \n    return prediction_vector\n\n\n\nscore()\nPerceptron.score(X, y) returns the accuracy of the perceptron as a number between 0 and 1, with 1 corresponding to perfect classification.\n\ndef score(self, X, y):\n    #preprocess X by padding with 1s\n    X_hat = np.append(X, np.ones((X.shape[0], 1)), 1)\n        \n    predictions = self.predict(X)\n        \n    accuracy = predictions == y \n        \n    accuracy = accuracy * 1\n        \n    accuracy = accuracy.mean()\n        \n    return accuracy\n\n\n\nhistory graph\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\nNameError: name 'p' is not defined\n\n\n\n\ndrawLine()\n\ndef draw_line(w, x_min, x_max):\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w_hat, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nNameError: name 'p' is not defined"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "Warmups.html",
    "href": "Warmups.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "## Warmup 2, Feb 20\n\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\ndef f(z):\n   return (-np.log(z))\n\ndef g(z):\n   return (-np.log(1-z))\n\nz = np.linspace(-10, 10, 100)\n\nplt.plot(z, f(z), color='red')\nplt.plot(z, g(z), color='blue')\n\nplt.show()\n\n/var/folders/tv/rp0f4yhd2yncf728pg446fzm0000gn/T/ipykernel_27747/2234136116.py:7: RuntimeWarning: invalid value encountered in log\n  return (-np.log(z))\n/var/folders/tv/rp0f4yhd2yncf728pg446fzm0000gn/T/ipykernel_27747/2234136116.py:10: RuntimeWarning: invalid value encountered in log\n  return (-np.log(1-z))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets, CSCI 0451.\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets, CSCI 0451.\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJulia Fairbank\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "features-regularization-live (1).html",
    "href": "features-regularization-live (1).html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "$$\n$$"
  },
  {
    "objectID": "features-regularization-live (1).html#feature-maps",
    "href": "features-regularization-live (1).html#feature-maps",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "Feature Maps",
    "text": "Feature Maps\nSuppose that we were able to extract from each point its distance from the origin. In 2d, we could take a point \\(\\mathbf{x}\\) and simply compute\n\\[\nr^2 = x_1^2 + x_2^2\\;.\n\\]\nWe could then make the classification based on the value of \\(r^2\\). In this data set, it looks like the classification rule that predicts \\(1\\) if \\(r^2 < 1\\) and \\(0\\) otherwise would be a pretty good one. The important insight here is that this is also a linear model, with linear predictor function\n\\[\n\\hat{y} = \\langle \\mathbf{r}, \\mathbf{w} \\rangle\\;,\n\\]\nand predicted labels \\(\\mathbb{1}[\\hat{y} < 0]\\).\nwhere \\(\\mathbf{r}= (r^2, 1)\\) and \\(\\mathbf{w}= (1, -1)\\). This means that we can use empirical risk minimization for this problem if we just transform the features \\(\\mathbf{X}\\) first! We need to compute a matrix \\(\\mathbf{R}\\) whose \\(i\\)th row is \\(\\mathbf{r}_i = (r^2_i, 1) = (x_{i1}^2 + x_{i2}^2, 1)\\), and then use this matrix in place of \\(\\mathbf{X}\\) for our classification task.\nThe transformation \\((x_1, x_2) \\mapsto (x_1^2 + x_2^2, 1)\\) is an example of a feature map.\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 1 A feature map \\(\\phi\\) is a function \\(\\phi:D \\rightarrow \\mathbb{R}^p\\), where \\(D\\) is the set of possible data values. If \\(d\\in D\\) is a data point, we call \\(\\phi(d) = \\mathbf{x}\\in \\mathbb{R}^p\\) the feature vector corresponding to \\(d\\). For a given feature map \\(\\phi\\), we define the map \\(\\Phi:D^n \\rightarrow \\mathbb{R}^{n\\times p}\\) as\n\\[\n\\Phi(\\mathbf{d}) = \\left(\\begin{matrix}\n     - & \\phi(d_1) & - \\\\\n     - & \\phi(d_2) & - \\\\\n     \\vdots & \\vdots & \\vdots \\\\\n     - & \\phi(d_n) & - \\\\\n\\end{matrix}\\right)\n\\]\nWe’ll often write\n\\[\n\\mathbf{X}= \\Phi(\\mathbf{d})\n\\]\nto say that \\(\\mathbf{X}\\) is the feature matrix for a data set \\(\\mathbf{d}\\).\n\n\n\nWe can think of feature maps in two ways:\nFeature maps can represent measurement processes. For example, maybe I am trying to classify penguins by species, based on physiological measurements. The real data is the penguin, and the measurements are how I represent that penguin with numbers. In this case, I might write my feature map as \\[\\phi(🐧) = (\\mathrm{height}, \\mathrm{weight}, \\text{bill length})\\] Here, \\(D\\) is a set of many penguins \\(D = \\{🐧_1, 🐧_2, 🐧_3, 🐧_4, 🐧_5, 🐧_6, 🐧_7\\}\\), and \\(d\\in D\\) is a specific penguin. The process of transforming an object into a vector via a feature map is often called vectorization as well, especially in the context of representing digital data as vectors. We often talk about vectorizing text and images for example; this can be done using feature maps.\nFeature maps can also represent data processing, which is more like our example above. There, we’re taking some data that’s already a vector and turning it into a DIFFERENT vector that we think will be helpful for our learning task."
  },
  {
    "objectID": "features-regularization-live (1).html#feature-maps-and-linear-separability",
    "href": "features-regularization-live (1).html#feature-maps-and-linear-separability",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "Feature Maps and Linear Separability",
    "text": "Feature Maps and Linear Separability\nWe often think of feature maps as taking us from a space in which the data is not linearly separable to a space in which it is. For example, consider the feature map\n\\[\n(x_1, x_2) \\maps_to (x_1^2, x_2^2)\\;.\n\\]\nThis map is sufficient to express the radius information, since we can represent the radius as\n\\[\nr^2 = \\langle (1, 1), (x_1^2, x_2^2) \\rangle\\;.\n\\]\nLet’s see how this looks. We’ll again show the failed linear separator, and we’ll also show a successful separator in a transformed feature space:\n\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_decision_regions(X, y, clf = LR, ax = axarr[0])\nscore = axarr[0].set_title(f\"Accuracy = {LR.score(X, y)}\")"
  },
  {
    "objectID": "features-regularization-live (1).html#feature-maps-in-practice",
    "href": "features-regularization-live (1).html#feature-maps-in-practice",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "Feature Maps in Practice",
    "text": "Feature Maps in Practice\nGoing back to our example of trying to classify the two nested circles, we could just compute the radius. In practice, however, we don’t really know which features are going to be most useful, and so we just compute a set of features. In our case, the square of the radius is an example of a polynomial of degree 2: \\[\nr^2 = x_1^2 + x_2^2\\;.\n\\] So, instead of just assuming that the radius is definitely the right thing to compute, we more frequently just compute all the monomials of degree 2 or lower. If \\(\\mathbf{x}= (x_1, x_2)\\), then this is\n\\[\n\\phi(\\mathbf{x}_i) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2)\\;.\n\\]\nWe then use a linear model to solve the empirical risk minimization problem\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{w} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle, y_i)\\;.\n\\]\nThe important point to keep track of is that the new feature matrix \\(\\mathbf{X}' = \\Phi(\\mathbf{X})\\) has more columns than \\(\\mathbf{X}\\). In this case, for example, \\(\\mathbf{X}\\) had just 2 columns but \\(\\Phi(\\mathbf{X})\\) has 6. This means that \\(\\hat{\\mathbf{w}}\\) has 6 components, instead of 2!\nLet’s now run logistic regression with degree-2 polynomial features on this data set. The most convenient way to make this happen in the scikit-learn framework is with at Pipeline. The Pipeline first applies the feature map and then calls the model during both fitting and evaluation. We’ll wrap the pipeline in a simple function for easy reuse.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef poly_LR(degree, **kwargs):\n    plr = Pipeline([(\"poly\", PolynomialFeatures(degree = degree)),\n                    (\"LR\", LogisticRegression(**kwargs))])\n    return plr\n\ndef viz_plr(plr, X, y):  \n    plot_decision_regions(X, y, clf = plr)\n    score = plt.gca().set_title(f\"Accuracy = {plr.score(X, y)}\")  \n\n\n# \n# \n#\n\nLet’s check the coefficients of the model:\n\n#\n\nNotice that two coefficients are much larger than the others, and approximately equal. These are the coefficients for the features \\(x_1^2\\) and \\(x_2^2\\). The fact that these are approximately equal means that our model is very close to using the square radius \\(r^2 = x_1^2 + x_2^2\\) for this data, just like we’d expect. The benefit is that we didn’t have to hard-code that in; the model just detected on its own the right pattern to find.\nPart of the reason this might be beneficial is that for some data sets, we might not really know what specific features we should try. For example, here’s another one where a linear classifier doesn’t do so great (degree 1 corresponds to no transformation of the features).\n\nnp.random.seed(123)\nX, y = make_moons(200, shuffle = True, noise = 0.2)\n\n# \n# \n#\n\nIt’s not as obvious that we should use the radius or any other specific feature for our feature map. Fortunately we don’t need to think too much about it – we can just increase the degree and let the model figure things out:\n\n# \n# \n#\n\nMuch nicer!"
  },
  {
    "objectID": "features-regularization-live (1).html#generalization-feature-selection-regularization",
    "href": "features-regularization-live (1).html#generalization-feature-selection-regularization",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "Generalization, Feature Selection, Regularization",
    "text": "Generalization, Feature Selection, Regularization\nSo, why don’t we just use as many features as it takes to get perfect accuracy on the training data? Here’s an example where we get perfect accuracy on the training data:\n\n# \n# \n#\n\nI’ve had to change some parameters to the LogisticRegression in order to ensure that it fully ran the optimization procedure for this many polynomials.\nThe problem here is that, although this classifier might achieve perfect training accuracy, it doesn’t really look like it’s captured “the right” pattern. This means that if we ask it to classify similar new data, it’s unlikely to do as well:\n\n#\n#\n#\n\nWhoops! We have overfit: our model was so flexible that it was able to learn both some real patterns that we wanted it to learn and some noise that we didn’t. As a result, when it made a prediction on new data, the model’s predictions were imperfect, reflecting the noise it learned in the training process.\nIn machine learning practice, we don’t actually want our models to get perfect scores on the training data – we want them to generalize to new instances of unseen data. Overfitting is one way in which a model can fail to generalize.\nLet’s do an experiment in which we see what happens to the model’s generalization ability when we increase the number of polynomial features:\n\nimport pandas as pd\nnp.random.seed()\n\ndegs = range(0, 11)\n\ndf = pd.DataFrame({\"deg\": [], \"train\" : [], \"test\" : []})\n\nfor rep in range(10):\n    X_train, y_train = make_moons(100, shuffle = True, noise = .4)\n    X_test, y_test = make_moons(100, shuffle = True, noise = .4)\n\n    for deg in degs:\n        plr = poly_LR(degree = deg, penalty = \"none\", max_iter = 1e3)\n        plr.fit(X_train, y_train)\n\n        to_add = pd.DataFrame({\"deg\" : [deg],\n                               \"train\" : [plr.score(X_train, y_train)],\n                               \"test\" : [plr.score(X_test, y_test)]})\n\n        df = pd.concat((df, to_add))\n        \nmeans = df.groupby(\"deg\").mean().reset_index()\n\nplt.plot(means[\"deg\"], means[\"train\"], label = \"training\")\nplt.plot(means[\"deg\"], means[\"test\"], label = \"validation\")\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Degree of polynomial feature\",\n              ylabel = \"Accuracy (mean over 20 runs)\")\n\nWe observe that there is an optimal number of features for which the model is most able to generalize: around 3 or so. More features than that is actually harmful to the model’s predictive performance.\nSo, one way to promote generalization is to try to find “the right” or “the right number” of features and use them for prediction. This problem is often called feature selection.\nAnother common approach to avoid overfitting is called regularization. In regularization, we actually modify the empirical risk objective function that is to be minimized. Instead of trying to minimize Equation 1, we instead consider the modified objective function \\[\nL'(\\mathbf{w}) = L(\\mathbf{w}) + \\lambda R(\\mathbf{w})\\;,\n\\] where \\(\\lambda\\) is a regularization strength and \\(R(\\mathbf{w})\\) is a regularization function that aims to influence the entries of \\(\\mathbf{w}\\) in some way. Common choices of regularization function include the Euclidean norm \\(R(\\mathbf{w}) = \\lVert \\mathbf{w} \\rVert_2^2\\) and the \\(\\ell_1\\) norm \\(R(\\mathbf{w}) = \\sum_{j = 1}^p \\lvert x_j \\rvert\\). To see regularization in action, let’s go back to our logistic regression model with a large number of polynomial features. We can see the presence of overfitting in the excessive “wiggliness” of the decision boundary.\n\n#\n#\n#\n\nFortunately for us, we can actually use regularization directly from inside the scikit-learn implementation of LogisticRegression. We specify the penalty (the \\(\\ell_1\\) regularization), the strength of the penalty (in the scikit-learn implementation, you specify \\(C = \\frac{1}{\\lambda}\\) so that larger \\(C\\) means less regularization) and the optimization solver (not all solvers work with all penalties).\n\n#\n#\n#\n\nThis looks more likely to generalize! We can also increase the regularization:\n\n#\n#\n#\n\nor decrease it:\n\n#\n#\n#\n\nLike last time, we can conduct a search (often called a grid-search) to find the best value of the regularization strength for a given problem. We’ll hold fixed the number of features, and instead vary the regularization strength:\n\nnp.random.seed()\n\nC = 10.0**np.arange(-4, 5)\n\ndf = pd.DataFrame({\"C\": [], \"train\" : [], \"test\" : []})\n\nfor rep in range(10):\n    X_train, y_train = make_moons(100, shuffle = True, noise = .3)\n    X_test, y_test = make_moons(100, shuffle = True, noise = .3)\n\n    for c in C:\n        plr = poly_LR(degree = 15, penalty = \"l1\", solver = \"liblinear\", C = c)\n\n        plr.fit(X_train, y_train)\n\n        to_add = pd.DataFrame({\"C\" : [c],\n                               \"train\" : [plr.score(X_train, y_train)],\n                               \"test\" : [plr.score(X_test, y_test)]})\n\n        df = pd.concat((df, to_add))\n     \nmeans = df.groupby(\"C\").mean().reset_index()\n\nplt.plot(means[\"C\"], means[\"train\"], label = \"training\")\nplt.plot(means[\"C\"], means[\"test\"], label = \"validation\")\nplt.semilogx()\nplt.legend()\nlabs = plt.gca().set(xlabel = \"C\",\n              ylabel = \"Accuracy (mean over 20 runs)\")\n\nUsing 15 features, it looks like a regularization strength of approximately \\(C = 10\\) is a good choice for this problem."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog: Julia Fairbank is a senior at Middlebury College pursuing a double major in Computer Science and Philosophy. She created this blog to post about CS 0451: Machine Learning."
  },
  {
    "objectID": "goal-setting.html",
    "href": "goal-setting.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "[Julia Fairbank]\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\n[I would love to specialize on implemtation and focus on my coding skills!]\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI would love to submit at least an attempt on all of the blog posts. I hope to attempt to revise each blog post by utilizing Student Hours or PeerHelp.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI would like to make strong attempts on all of the warmups and be prepared to present them to my peers. I would like to attend all of the classes that I am physically able to. I hope to attend Student Hours or Peer Help as much as I can in order to stay up to date with my coursework and assignments.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI hope to set a regular time each week to work with my partners and submit all deliverables/milestones on time. I hope to take the lead on a certain section/algorithm within the project."
  },
  {
    "objectID": "Gradient Descent.html",
    "href": "Gradient Descent.html",
    "title": "Julia Fairbank CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np\n\nalpha = 0.001 #learning rate\n\ndone = False\nprev_loss = np.inf #start off the loss\n\ndef f(x):\n    return np.sin(x[0]*x[1]\n\ndef gradient(w1, w2):\n    while not done:\n                  \n    \n                \n                  \n\nSyntaxError: invalid syntax (3313188621.py, line 8)"
  }
]